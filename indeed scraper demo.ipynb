{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found existing installation: webdriver-manager 4.0.1\n",
      "Uninstalling webdriver-manager-4.0.1:\n",
      "  Would remove:\n",
      "    /Users/arifrahman/Project/scraper/JobMiner/.venv/lib/python3.12/site-packages/webdriver_manager-4.0.1.dist-info/*\n",
      "    /Users/arifrahman/Project/scraper/JobMiner/.venv/lib/python3.12/site-packages/webdriver_manager/*\n",
      "Proceed (Y/n)? ^C\n",
      "\u001B[31mERROR: Operation cancelled by user\u001B[0m\u001B[31m\n",
      "\u001B[0m"
     ]
    }
   ],
   "source": [
    "!pip3 uninstall webdriver_manager\n"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-08T19:20:48.734745Z",
     "start_time": "2024-06-08T19:20:48.623662Z"
    }
   },
   "source": [
    "import os\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.chrome.service import Service\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.chrome.options import Options\n",
    "from webdriver_manager.chrome import ChromeDriverManager\n",
    "from pymongo import MongoClient\n",
    "from dotenv import load_dotenv\n",
    "import time"
   ],
   "outputs": [],
   "execution_count": 1
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load environment variables from .env file\n",
    "load_dotenv()\n",
    "MONGO_DB_URL = os.getenv(\"MONGO_DB_URL\")"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-08T19:21:30.165086Z",
     "start_time": "2024-06-08T19:21:30.161656Z"
    }
   },
   "source": [
    "# Setup Chrome options\n",
    "chrome_options = Options()\n",
    "# chrome_options.add_argument(\"--headless\")  # Ensure GUI is off\n",
    "chrome_options.add_argument(\"--no-sandbox\")\n",
    "chrome_options.add_argument(\"--disable-dev-shm-usage\")\n"
   ],
   "outputs": [],
   "execution_count": 4
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-08T19:20:58.596696Z",
     "start_time": "2024-06-08T19:20:58.292133Z"
    }
   },
   "source": [
    "# Set path to chromedriver as per your configuration\n",
    "webdriver_service = Service(ChromeDriverManager().install())\n",
    "\n"
   ],
   "outputs": [],
   "execution_count": 2
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-08T20:42:19.275290Z",
     "start_time": "2024-06-08T20:42:18.645095Z"
    }
   },
   "source": [
    "# Choose Chrome Browser\n",
    "driver = webdriver.Chrome(service=webdriver_service, options=chrome_options)\n",
    "\n"
   ],
   "outputs": [],
   "execution_count": 15
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-08T20:42:28.332276Z",
     "start_time": "2024-06-08T20:42:26.809221Z"
    }
   },
   "source": [
    "# URL of the job listings page\n",
    "url = 'https://www.indeed.com/jobs?q=data+engineer&l=California'\n",
    "driver.get(url)\n",
    "\n"
   ],
   "outputs": [],
   "execution_count": 16
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Connect to MongoDB\n",
    "client = MongoClient(MONGO_DB_URL)\n",
    "db = client['job_miner']\n",
    "collection = db['jobs_link']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Find all job postings\n",
    "job_postings = driver.find_elements(By.CLASS_NAME, 'job_seen_beacon')\n",
    "\n",
    "print(job_postings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract information from each job posting\n",
    "for job in job_postings:\n",
    "    # Extract job title\n",
    "    title_elem = job.find_element(By.CLASS_NAME, 'jobTitle')\n",
    "    title = title_elem.text.strip() if title_elem else 'N/A'\n",
    "    \n",
    "    # Extract company name    \n",
    "    company_elem = job.find_element(By.CSS_SELECTOR, \"span[data-testid='company-name']\")\n",
    "    company = company_elem.text.strip() if company_elem else 'N/A'\n",
    "    \n",
    "    # Extract job location\n",
    "    location_elem = job.find_element(By.CSS_SELECTOR, \"div[data-testid='text-location']\")\n",
    "    location = location_elem.text.strip() if location_elem else 'N/A'\n",
    "    \n",
    "    # Extract job link\n",
    "    joblink_elem = job.find_element(By.CLASS_NAME, 'jobTitle').find_element(By.CLASS_NAME, 'jcs-JobTitle').get_attribute('href')\n",
    "    joblink = joblink_elem.strip() if joblink_elem else 'N/A'\n",
    "    \n",
    "    # Print job information\n",
    "    print(f'Job Title: {title}')\n",
    "    print(f'Company: {company}')\n",
    "    print(f'Location: {location}')\n",
    "    print(f'Job Link: {joblink_elem}')\n",
    "    print('-' * 40)\n",
    "\n",
    "    # Create a dictionary to store the job data\n",
    "    job_data = {\n",
    "        'title': title,\n",
    "        'company': company,\n",
    "        'location': location,\n",
    "        'job_link': joblink\n",
    "    }\n",
    "    \n",
    "    # Insert the job data into MongoDB\n",
    "    collection.insert_one(job_data)\n"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-08T20:42:11.143455Z",
     "start_time": "2024-06-08T20:42:10.885173Z"
    }
   },
   "source": [
    "# Close the browser\n",
    "driver.quit()\n"
   ],
   "outputs": [],
   "execution_count": 14
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the base URL and number of pages to scrape\n",
    "base_url = 'https://www.indeed.com/jobs?q=data+engineer&l=California'\n",
    "num_pages = 10\n",
    "\n",
    "for page in range(num_pages):\n",
    "    url = f'{base_url}&start={page * 10}'\n",
    "    driver.get(url)\n",
    "    time.sleep(10)  # wait for the page to load\n",
    "\n",
    "    # Find all job postings\n",
    "    job_postings = driver.find_elements(By.CLASS_NAME, 'job_seen_beacon')\n",
    "\n",
    "    # Extract information from each job posting\n",
    "    for job in job_postings:\n",
    "        # Extract job title\n",
    "        title_elem = job.find_element(By.CLASS_NAME, 'jobTitle')\n",
    "        title = title_elem.text.strip() if title_elem else 'N/A'\n",
    "        \n",
    "        # Extract company name    \n",
    "        company_elem = job.find_element(By.CSS_SELECTOR, \"span[data-testid='company-name']\")\n",
    "        company = company_elem.text.strip() if company_elem else 'N/A'\n",
    "        \n",
    "        # Extract job location\n",
    "        location_elem = job.find_element(By.CSS_SELECTOR, \"div[data-testid='text-location']\")\n",
    "        location = location_elem.text.strip() if location_elem else 'N/A'\n",
    "        \n",
    "        # Extract job link\n",
    "        joblink_elem = job.find_element(By.CLASS_NAME, 'jobTitle').find_element(By.CLASS_NAME, 'jcs-JobTitle').get_attribute('href')\n",
    "        joblink = joblink_elem.strip() if joblink_elem else 'N/A'\n",
    "        \n",
    "        # Print job information\n",
    "        print(f'Job Title: {title}')\n",
    "        print(f'Company: {company}')\n",
    "        print(f'Location: {location}')\n",
    "        print(f'Job Link: {joblink}')\n",
    "        print('-' * 40)\n",
    "\n",
    "        # Create a dictionary to store the job data\n",
    "        job_data = {\n",
    "            'title': title,\n",
    "            'company': company,\n",
    "            'location': location,\n",
    "            'job_link': joblink\n",
    "        }\n",
    "        \n",
    "        # Insert the job data into MongoDB\n",
    "        collection.insert_one(job_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Job Title: Data Engineer -\n",
      "Company: Kandji\n",
      "Location: San Francisco, CA\n",
      "Job Link: https://www.indeed.com/rc/clk?jk=4b8606224f6c913b&bb=ZIMRsFk2_AHPe4UietgcKB9eKSvgtq7xxccsGfhvFPotIV0PpOazgMBKfrvmDZWK04gjK0gZesaJnN0eNcRlqBvEKXQcAmks6POrMt2CEkMUuKxbtZxD1VjUbyibHtKq&xkcb=SoCm67M3AbjOLl2M2p0LbzkdCdPP&fccid=e7e14440c198dd78&vjs=3\n",
      "Details: {}\n",
      "Description: Profile insights\n",
      "Find out how your skills align with the job description\n",
      "Skills\n",
      "Do you have experience in SaaS?\n",
      "Yes\n",
      "No\n",
      "Job details\n",
      "Here’s how the job details align with your profile\n",
      ".\n",
      "Pay\n",
      "$120,000 - $130,000 a year\n",
      "Job type\n",
      "Full-time\n",
      "Location\n",
      "San Francisco, CA\n",
      "Benefits\n",
      "Pulled from the full job description\n",
      "401(k) 4% Match\n",
      "401(k) matching\n",
      "Dental insurance\n",
      "Gym membership\n",
      "Health insurance\n",
      "Paid parental leave\n",
      "Paid time off\n",
      "Show more\n",
      "Full job description\n",
      "About Kandji\n",
      "\n",
      "Kandji is the Apple Device Management and Security Platform. Kandji empowers companies to manage and secure Apple devices in the enterprise and at scale. By centrally securing and managing Mac, iPhone, iPad, and Apple TV devices, IT and InfoSec teams can save countless hours of manual, repetitive work with features like one-click compliance templates and more than 150 pre-built automations, apps, and workflows.\n",
      "\n",
      "Device Harmony is our vision for tearing down the wall between IT and InfoSec to keep every Apple user secure and productive, using connected intelligence and automation. By choosing a career with Kandji, you will play an integral role in contributing to making our vision a reality. Backed by world-class investors such as Tiger Global, Greycroft, B Capital Group, Okta Ventures, the Spruce House Partnership, and First Round Capital, Kandji has raised over $100+M in capital to date.\n",
      "\n",
      "Trusted by industry leaders, Kandji’s rapidly growing customer base includes companies like Ramp, Notion, Netskope, Noom, Turo, Groupon, VoxMedia, and more.\n",
      "\n",
      "Recognized for its award-winning products, Kandji was recently named the #1 fastest-growing app in Okta’s 2023 Businesses at Work Report and a G2 Best Software 2023 Award Winner for Fastest Growing Products!\n",
      "\n",
      "The Opportunity\n",
      "\n",
      "As a Data Engineer, you are passionate and drive excellence across data ingestion, preparation, modeling and quality. The successful candidate will drive the organization's design of high performance data models, helping us effectively get the most from our data.\n",
      "How You'll Make A Difference Day To Day\n",
      "Collaborate with stakeholders to solve problems by modeling complex datasets to fit their needs\n",
      "Identify, design, and implement internal process improvements; automate manual processes where appropriate. Optimize how data gets delivered, and scale our data systems where needed\n",
      "Design processes for data observability, usage metrics, and monitoring of the platform\n",
      "Create data tools for the organization to make data-driven decisions\n",
      "Be an active contributor to architectural decisions as we enhance our data platform and infrastructure\n",
      "\n",
      "Required to work on-site 3 days a week (Tuesday, Wednesday, Thursday). Managers may require additional on-site days.\n",
      "Minimum Qualifications:\n",
      "3+ years of strong data engineering design/development experience in building large-scale distributed data platforms/products\n",
      "Knowledge of Python, SQL\n",
      "Experience with dbt and data modeling, especially incremental models\n",
      "Experience designing and implementing data ingestion platforms, interacting with multiple third-party data sources, and assembling them into actionable structures\n",
      "Experience in developing data architectures within greenfield efforts\n",
      "Experience with both structured and unstructured datasets\n",
      "Ability to establish and maintain relationships with key stakeholders\n",
      "Preferred Qualifications:\n",
      "Experience with a Security related SaaS product\n",
      "Familiarity with Apache Kafka, or Kinesis\n",
      "Familiarity with applying Data Science algorithms and/or ML ops\n",
      "Experience with a variety of data serialization formats (Avro, Parquet, JSON, Protobuf, etc)\n",
      "Familiar with workflow management systems (Airflow, Argo Workflows, etc)\n",
      "$120,000 - $130,000 a year\n",
      "\n",
      "These requirements are for the strongest, ideal candidate. Even if you do not outperform every bullet point, Kandji encourages you to apply. We promote a diverse, equitable, and inclusive culture and recognize that even the strongest candidates won’t have all desired experiences and qualifications.\n",
      "\n",
      "Benefits & Perks\n",
      "\n",
      "Competitive salary\n",
      "100% individual and dependent medical + dental + vision coverage\n",
      "401(k) with a 4% company match\n",
      "20 days PTO\n",
      "14 paid holidays per year\n",
      "10 health and wellness days per year\n",
      "Kandji Wellness Week Off July 1 - July 5, 2024\n",
      "Equity for full-time employees\n",
      "12 weeks of paid leave for new parents\n",
      "Paid Family and Medical Leave\n",
      "Modern Health - Mental Health Benefits - Individual and Dependents\n",
      "Monthly Utilities stipend\n",
      "Gym Membership\n",
      "Lunch 3 Days/Week\n",
      "Exciting opportunities for career growth\n",
      "An outstanding, inclusive culture\n",
      "\n",
      "We are excited to be serving a significant need for a fast-growing market, and are proud of the high-performing team we have brought together so far. If you’re someone who wants to engage in new, exciting projects that will challenge your skills in the best way possible, we would love to connect with you.\n",
      "\n",
      "At Kandji we believe in fostering an inclusive environment in which employees feel encouraged to share their unique perspectives, leverage their strengths, and act authentically. We know that diverse teams are strong teams, and welcome those from all backgrounds and varying experiences.\n",
      "\n",
      "Kandji is proud to be an equal opportunity employer committed to diversity and inclusion in the workplace. Qualified applicants will be considered for employment without regard to race, color, religion, national origin, age, sex, sexual orientation, gender identity, physical or mental disability, protected veteran or military status or any other status protected by applicable law.\n",
      "----------------------------------------\n",
      "Job Title: Data Engineer\n",
      "Company: Rigil Corporation\n",
      "Location: San Francisco, CA 94110 \n",
      "(Mission area)\n",
      "Job Link: https://www.indeed.com/rc/clk?jk=667a2e52e1877cc4&bb=ZIMRsFk2_AHPe4UietgcKA_cfYWaBmqJ-ualQSyrYmYE6L7NiuaM5QQf0ykZxNw9y6tRpW_USb4Qkp4EOLVK4dV0aO_K1NMCKumGIgXOX_e7iI_gqRrnciUEH4TknQR_&xkcb=SoAS67M3AbjOLl2M2p0KbzkdCdPP&fccid=9a3099696e9e959e&vjs=3\n",
      "Details: {}\n",
      "Description: Profile insights\n",
      "Find out how your skills align with the job description\n",
      "Skills\n",
      "Do you have experience in Scripting?\n",
      "Yes\n",
      "No\n",
      "Education\n",
      "Do you have a Bachelor's degree?\n",
      "Yes\n",
      "No\n",
      "Job details\n",
      "Here’s how the job details align with your profile\n",
      ".\n",
      "Pay\n",
      "$90,000 - $105,000 a year\n",
      "Job type\n",
      "Full-time\n",
      "Location\n",
      "San Francisco, CA 94110\n",
      "Full job description\n",
      "Role: Data Engineer\n",
      "\n",
      "About Rigil:\n",
      "Rigil is an award-winning, woman-owned, small business that specializes in technology consulting, strategy consulting and product development. We value teamwork and strive to build strong leaders.\n",
      "\n",
      "Location: San Francisco, CA\n",
      "Job Type: Full Time\n",
      "\n",
      "Data Engineer is integral to evolving Vision and its underlying data and AI platform, which forms the backbone of our business. Vision is much more than a tool; it's the operational core and a vital integration layer that connects our installed hardware, connectivity network, and customers. This platform streamlines the marketing, delivery, installation, and maintenance of our energy products globally.\n",
      "\n",
      "Your work will be pivotal in enabling our platform to scale, be intelligent and provide secure and easy access to data that customers can trust. By enhancing Vision, you will empower our international partners, simplifying and improving their ability to connect with customers and support our energy appliances. Your efforts will ensure that our data and analytics platform is not only agile and responsive but also anticipates and adapts to the evolving needs of diverse markets and stakeholders.\n",
      "\n",
      "In this foundational phase of our platform, your role is central to adopting an AI-centric strategy. You will be at the forefront of building the next stage of Vision, focusing on new capabilities that embed conversational interfaces that dynamically learn and adapt to customer needs. These interfaces will not only enhance user experiences but also glean insightful data about our customers and business operations.\n",
      "\n",
      "This position demands a unique combination of technical skill, pragmatic orientation, data- centric thinking, and an intuitive sense for how Large Language Models are poised to change how we approach software, both how it works, and how we develop it.\n",
      "\n",
      "Responsibilities\n",
      "Create and maintain optimal data pipeline architecture.\n",
      "Build the infrastructure required for optimal extraction, transformation, and loading of data from a wide variety of data sources using SQL, NoSQL and AWS ‘big data’ technologies.\n",
      "Build analytics (AI & ML) tools that utilize the data pipeline to provide actionable insights into customer acquisition, device operations and other key business performance metrics.\n",
      "Keep our data separated and secure across tenants and geographical boundaries through data segmentation and infrastructure.\n",
      "Manage databases effectively using PostgreSQL, ensuring data integrity and performance.\n",
      "Utilize AWS for continuous deployment and cloud services management.\n",
      "Work within Agile development frameworks, contributing to all phases of the software development lifecycle.\n",
      "Innovate and contribute to a technology roadmap that aligns with our mission of providing sustainable energy solutions.\n",
      "Qualifications\n",
      "Bachelor’s degree in computer science or related field; Advanced degree preferred.\n",
      "At least 5 years of data engineering experience.\n",
      "Experience with object-oriented/object function scripting languages: Python, Java, C++, Scala, etc.\n",
      "Experience building and optimizing ‘big data’ data pipelines, architectures, and data sets.\n",
      "Experience with data pipeline and workflow management tools: Azkaban, Luigi, Airflow, etc.\n",
      "Experience with AWS cloud services: EC2, EMR, RDS, Redshift.\n",
      "Working knowledge of message queuing, stream processing, and highly scalable ‘big data’ data stores.\n",
      "Working knowledge of MLOps.\n",
      "Familiarity with IoT platforms and integrating AI tools in data applications.\n",
      "----------------------------------------\n",
      "Job Title: SAP Data Center Engineer\n",
      "Company: SAP\n",
      "Location: Hybrid work in Palo Alto, CA 94304\n",
      "Job Link: https://www.indeed.com/rc/clk?jk=58791e0a5ad154fd&bb=ZIMRsFk2_AHPe4UietgcKEJ1kX6STmPp9sJ9k-usE2FnW5iAyvE9VqopNQqoJ4Czq65BBF20anba4N9wgVVXt2NHyWHZBnbKbDNWmqKdW9VDYEHklSIA3Q%3D%3D&xkcb=SoCP67M3AbjOLl2M2p0JbzkdCdPP&fccid=156c7820d1988640&vjs=3\n",
      "Details: {}\n",
      "Description: Profile insights\n",
      "Find out how your skills align with the job description\n",
      "Skills\n",
      "Do you have experience in Data center experience?\n",
      "Yes\n",
      "No\n",
      "Education\n",
      "Do you have a Bachelor's degree?\n",
      "Yes\n",
      "No\n",
      "Job details\n",
      "Here’s how the job details align with your profile\n",
      ".\n",
      "Job type\n",
      "Full-time\n",
      "Benefits\n",
      "Pulled from the full job description\n",
      "Referral program\n",
      "Full job description\n",
      "We help the world run better\n",
      "\n",
      "At SAP, we enable you to bring out your best. Our company culture is focused on collaboration and a shared passion to help the world run better. How? We focus every day on building the foundation for tomorrow and creating a workplace that embraces differences, values flexibility, and is aligned to our purpose-driven and future-focused work. We offer a highly collaborative, caring team environment with a strong focus on learning and development, recognition for your individual contributions, and a variety of benefit options for you to choose from.\n",
      "\n",
      "What You'll do:\n",
      "The successful candidate will be responsible for:\n",
      "Managing Santa Clara/Palo Alto Data Centers for all Build and Support activities.\n",
      "Guiding technicians on functional topics for build and support activities.\n",
      "Efficiently operating a co-location space and collaborating with the provider on infrastructure topics such as HVAC, UPS, Generator and security systems.\n",
      "Maintaining responsibility for the Asset Information Quality and Integrity\n",
      "Ensuring Data Center Infrastructure Availability\n",
      "Overseeing Data Center Security operations and Audit support\n",
      "Supervising the local 7x24 HW support model\n",
      "Assisting in Global Reporting and Transparency for Santa Clara/Palo Alto Data Centers\n",
      "Cultivating a quality mindset towards all activities\n",
      "\n",
      "What You Bring:\n",
      "The ideal candidate will come with:\n",
      "A Bachelor’s degree or higher in Computer Science, Information Management, Mathematics, Chemistry, Physics, or equivalent degree.\n",
      "At least 5+ years of experience in the data center field.\n",
      "Industry certificaitons like CDCP, CDCS, & CDCE.\n",
      "Excellent fluency in spoken and written English.\n",
      "\n",
      "Meet Your Team:\n",
      "In this position, you will be joining the Global Cloud Services, a shared service unit that provides the basic infrastructure services to all other units within SAP which includes network, datacenter, storage and backup, server management, license management, and supplier coordination. You will have an incredible opportunity to contribute as an IT Technology Data Center Engineer in our Data Center Management department.\n",
      "\n",
      "Data Center Management is an international team responsible for managing and standardizing all activities within the SAP Data Centers, both owned or rented. You will be a part of a global team working with innovative technologies, and will constantly be seeking new and innovative ideas, processes, and improvements. Your direct superior will be the Manager of the Americas Data Center Management. You will be working collectively towards technological advancements with a highly motivated and driven team across the world.\n",
      "\n",
      "Bring out your best\n",
      "SAP innovations help more than four hundred thousand customers worldwide work together more efficiently and use business insight more effectively. Originally known for leadership in enterprise resource planning (ERP) software, SAP has evolved to become a market leader in end-to-end business application software and related services for database, analytics, intelligent technologies, and experience management. As a cloud company with two hundred million users and more than one hundred thousand employees worldwide, we are purpose-driven and future-focused, with a highly collaborative team ethic and commitment to personal development. Whether connecting global industries, people, or platforms, we help ensure every challenge gets the solution it deserves. At SAP, you can bring out your best.\n",
      "\n",
      "We win with inclusion\n",
      "SAP’s culture of inclusion, focus on health and well-being, and flexible working models help ensure that everyone – regardless of background – feels included and can run at their best. At SAP, we believe we are made stronger by the unique capabilities and qualities that each person brings to our company, and we invest in our employees to inspire confidence and help everyone realize their full potential. We ultimately believe in unleashing all talent and creating a better and more equitable world.\n",
      "SAP is proud to be an equal opportunity workplace and is an affirmative action employer. We are committed to the values of Equal Employment Opportunity and provide accessibility accommodations to applicants with physical and/or mental disabilities. If you are interested in applying for employment with SAP and are in need of accommodation or special assistance to navigate our website or to complete your application, please send an e-mail with your request to Recruiting Operations Team: Careers@sap.com.\n",
      "For SAP employees: Only permanent roles are eligible for the SAP Employee Referral Program, according to the eligibility rules set in the SAP Referral Policy. Specific conditions may apply for roles in Vocational Training.\n",
      "\n",
      "EOE AA M/F/Vet/Disability\n",
      "Qualified applicants will receive consideration for employment without regard to their age, race, religion, national origin, ethnicity, age, gender (including pregnancy, childbirth, et al), sexual orientation, gender identity or expression, protected veteran status, or disability.\n",
      "\n",
      "Compensation Range Transparency: SAP believes the value of pay transparency contributes towards an honest and supportive culture and is a significant step toward demonstrating SAP’s commitment to pay equity. SAP provides the annualized compensation range inclusive of base salary and variable incentive target for the career level applicable to the posted role. The targeted combined range for this position is $112,300 - $190,900 (USD) USD. The actual amount to be offered to the successful candidate will be within that range, dependent upon the key aspects of each case which may include education, skills, experience, scope of the role, location, etc. as determined through the selection process. Any SAP variable incentive includes a targeted dollar amount and any actual payout amount is dependent on company and personal performance. Please reference this link for a summary of SAP benefits and eligibility requirements: SAP North America Benefits.\n",
      "\n",
      "Requisition ID: 393763 | Work Area:Information Technology | Expected Travel: 0 - 10% | Career Status: Professional | Employment Type: Regular Full Time | Additional Locations: #LI-Hybrid\n",
      "----------------------------------------\n",
      "Job Title: Data Engineer\n",
      "Company: HomeLight\n",
      "Location: San Francisco Bay Area, CA\n",
      "Job Link: https://www.indeed.com/rc/clk?jk=d47cb25f579d3d28&bb=ZIMRsFk2_AHPe4UietgcKEPjqImjzbVeAj5R52V1FwZ0fhsCN_RLiqQ8AirCceLvhuSjeI__L1b2FnCkF4UPrUZavnioPIlFoONyMIxStgVXTyxwhnyXMtTanKWhdeEo&xkcb=SoA767M3AbjOLl2M2p0IbzkdCdPP&fccid=599860fae1e46a8f&vjs=3\n",
      "Details: {}\n",
      "Description: Profile insights\n",
      "Find out how your skills align with the job description\n",
      "Skills\n",
      "Do you have experience in SQL?\n",
      "Yes\n",
      "No\n",
      "Location\n",
      "San Francisco Bay Area, CA\n",
      "Benefits\n",
      "Pulled from the full job description\n",
      "401(k)\n",
      "Commuter assistance\n",
      "Dental insurance\n",
      "Disability insurance\n",
      "Health insurance\n",
      "Life insurance\n",
      "Paid time off\n",
      "Show more\n",
      "Full job description\n",
      "***** This role is based in San Francisco, CA - Office days are Wed and Thur *****\n",
      "Who We Are\n",
      "We're building the future of real estate — today.\n",
      "HomeLight is the essential technology platform used by hundreds of thousands of homebuyers and sellers to partner with top real estate agents and win at any step of the real estate journey, whether that's finding a top agent, securing a competitive mortgage, or ensuring on-time, easy close.\n",
      "HomeLight facilitates billions of dollars of real estate on its platform every year. Our vision is a world where every real estate transaction is simple, certain, and satisfying for all. Our team breaks barriers every day while staying committed to HomeLight's goals and core values, which is a crucial element to our shared success.\n",
      "Who You Are\n",
      "We are building our Data Engineering team to tackle HomeLight's diverse, data challenges. This position is an excellent opportunity for an engineer that wants to own the development, optimization, and operation of our data pipeline, which collects, processes, and distributes data to a suite of HomeLight products and teams. You will provide mission-critical data to both our algorithms and internal users, refining our product and identifying new markets.\n",
      "What You'll Do Here\n",
      "Some projects you will work on:\n",
      "Optimize and execute on requests to pull, analyze, interpret and visualize data\n",
      "Partner with team leaders across the organization to build out and iterate on team, and individual performance metrics\n",
      "Optimize our data release processes, and partner with team leads to iterate on and improve existing data pipelines.\n",
      "Design and develop systems that ingest and transform our data streams using the latest tools.\n",
      "Design, build, and integrate new cutting-edge databases and data warehouses, develop new data schemas and figure out new innovative ways of storing and representing our data.\n",
      "Research, architect, build, and test robust, highly available and massively scalable systems, software, and services.\n",
      "What You Bring\n",
      "3 to 5+ years of Python and ETL experience, preferably Airflow\n",
      "Experience writing and executing complex SQL queries\n",
      "Experience building data pipelines and ETL design (implementation and maintenance)\n",
      "Scrum/Agile software development process.\n",
      "Bonus points for\n",
      "Familiarity with chatgpt, transcription analysis\n",
      "Familiarity with AWS, Elasticsearch, Django\n",
      "Experience setting up and managing internal API services.\n",
      "Experience working on a small team, ideally at a startup.\n",
      "Familiarity with the Amazon AWS ecosystem\n",
      "Benefits and Perks\n",
      "Medical (Anthem or Kaiser), Dental & Vision (Guardian)\n",
      "Long-Term Disability & Short-Term Disability, Hospital Indemnity Insurance (Guardian)\n",
      "401k (Guideline), Life Insurance (Guardian) & Pet Insurance (Nationwide)\n",
      "Commuter benefits are offered in certain locations\n",
      "PTO, including Volunteer Days to give back to your community\n",
      "Annual Anniversary Perks, including professional development and sabbaticals!\n",
      "HomeLight Services to help you with buying and selling your home\n",
      "The following compensation information is provided to comply with job posting disclosure requirements in Colorado, New York, Washington, and California.\n",
      "Base Pay Range: $150,000.00 - $180,000.00, base pay will vary depending on several factors, such as the position, location, qualifications of the individual, market conditions, and other operational business requirements.\n",
      "Let's chat!\n",
      "HomeLight is an equal opportunity employer dedicated to building an inclusive and diverse workforce, providing employees with a work environment free of discrimination and harassment. All employment decisions are based on business needs, job requirements and individual qualifications, without regard to race, color, religion or belief, national, social or ethnic origin, sex (including pregnancy), age, physical, mental or sensory disability, HIV Status, sexual orientation, gender identity and/or expression, marital, civil union or domestic partnership status, past or present military service, family medical history or genetic information, family or parental status, or any other status protected by the laws or regulations in the locations where we operate. We will provide accommodations during the recruitment process upon request and any accommodation will be addressed confidentially.\n",
      "----------------------------------------\n",
      "Job Title: Data Engineer\n",
      "Company: Office of Energy Infrastructure Safety\n",
      "Location: Hybrid work in Sacramento, CA 95814\n",
      "Job Link: https://www.indeed.com/rc/clk?jk=4cd5ec18cb6fc934&bb=ZIMRsFk2_AHPe4UietgcKNkgrNe5fDAejRZ1IDId4ifYVmSkmo5O7nFuYfQK0foWG3-8p-nIuQMds2sv7oq3Y963gZ427s8oPkEar3gATqMaRY6aUGIkv4-tjnS17ypt&xkcb=SoC167M3AbjOLl2M2p0PbzkdCdPP&fccid=05927d043678be61&cmp=Office-of-Energy-Infrastructure-Safety&ti=Data+Engineer&vjs=3\n",
      "Details: {}\n",
      "Description: Profile insights\n",
      "Find out how your skills align with the job description\n",
      "Skills\n",
      "Do you have experience in SQL?\n",
      "Yes\n",
      "No\n",
      "Job details\n",
      "Here’s how the job details align with your profile\n",
      ".\n",
      "Pay\n",
      "$7,201 - $9,013 a month\n",
      "Job type\n",
      "Permanent\n",
      "Full-time\n",
      "Shift and schedule\n",
      "Monday to Friday\n",
      "Benefits\n",
      "Pulled from the full job description\n",
      "401(k)\n",
      "Dental insurance\n",
      "Employee discount\n",
      "Health insurance\n",
      "Paid time off\n",
      "Professional development assistance\n",
      "Retirement plan\n",
      "Show more\n",
      "Full job description\n",
      "Are you looking to make a difference and join a highly motivated team working on issues critical to ensuring the safety and sustainability of California? The Data Analytics Division is seeking a Research Data Specialist II to help enable data-supported decision making.\n",
      "Under the general direction of the Data Research Supervisor II, Data Analytics Division, the incumbent will independently perform statistical analysis, database operations, and dashboard development associated the assessment of wildfire mitigation strategies, such as the composition and magnitude of current and future trends, relevant for utility planning, programs, projects, underground safety and other wildfire mitigation-related activities. The incumbent is distinguished as a consultant and expert to management on procedural and policy matters involving the use and analysis of wildfire mitigation and operational data. Data Research Specialist II (RDS II) will operate under general direction, playing a pivotal role in developing and maintaining the availability, security, and performance of our database infrastructure. The RDS II will also contribute to dashboarding efforts, collaborating with cross-functional teams to develop and maintain data visualization dashboards.\n",
      "The Data Analytics Division supports the operational needs of the Office of Energy Infrastructure Safety. The division creates, maintains, distributes, and operationalizes information to enable data-supported decisions that are faster, more consistent, and more transparent. Energy Safety values strong analytical and communication skills and seeks individuals who perform well in team environments.\n",
      "To learn more and apply to this position please follow the link:\n",
      "https://calcareers.ca.gov/CalHrPublic/Jobs/JobPosting.aspx?JobControlId=419467\n",
      "Final Filing Date: 4/30/2024\n",
      "Job Control Number: 419467\n",
      "Job Types: Full-time, Permanent\n",
      "Pay: $7,201.00 - $9,013.00 per month\n",
      "Benefits:\n",
      "401(k)\n",
      "Dental insurance\n",
      "Employee discount\n",
      "Health insurance\n",
      "Paid time off\n",
      "Professional development assistance\n",
      "Retirement plan\n",
      "Vision insurance\n",
      "Experience level:\n",
      "5 years\n",
      "Schedule:\n",
      "Monday to Friday\n",
      "Work Location: Hybrid remote in Sacramento, CA 95814\n",
      "----------------------------------------\n",
      "Job Title: Data Analytics Engineer\n",
      "Company: Willow Innovations\n",
      "Location: Remote in Mountain View, CA\n",
      "Job Link: https://www.indeed.com/rc/clk?jk=68f93dee1201376a&bb=ZIMRsFk2_AHPe4UietgcKEJ1kX6STmPpdCmQkxLska33eutX98jdAP78rfOr8b3cIPPgmzeJNd4Bwy3MB3YRlPPTsokE9AtmJJ-5MBOB-Gc1wBUMvGBwGeaiUp8zosbT&xkcb=SoAB67M3AbjOLl2M2p0ObzkdCdPP&fccid=8047209135126d31&vjs=3\n",
      "Details: {}\n",
      "Description: Profile insights\n",
      "Find out how your skills align with the job description\n",
      "Skills\n",
      "Do you have experience in Tableau?\n",
      "Yes\n",
      "No\n",
      "Benefits\n",
      "Pulled from the full job description\n",
      "401(k)\n",
      "Dental insurance\n",
      "Health insurance\n",
      "Parental leave\n",
      "Stock options\n",
      "Vision insurance\n",
      "Full job description\n",
      "At Willow Innovations, Inc., our vision is to create a better world for moms. We are focused on identifying where moms need support in the motherhood journey, and building hardware- and software-based solutions around their needs.\n",
      "In 2014, we created the first-ever cord-free Willow™ Wearable Breast Pump, revolutionizing the industry. Fast forward to today, we are building a portfolio of physical and digital products to solve mom's most meaningful problems and improve their lives.\n",
      "Willow Innovations, Inc. sits at the unique and exciting intersection of HealthTech, FemTech, Consumer Goods, and Med Device. Our potential for scale and impact is extraordinary—we're just getting started.\n",
      "We're looking for talented, passionate team players to help us deliver our mission and vision. If this sounds like you, we'd love to hear from you!\n",
      "Willow is seeking a Data Analytics Engineer. You will be an integral part of our Willow product team to continue to innovate for our moms. You will collaborate closely with cross-functional teams, including product management, marketing, and engineering, to extract actionable insights from large datasets. By leveraging your expertise in data analysis and statistical modeling, you will help shape product development initiatives, optimize user experiences, and improve overall product performance.\n",
      "\n",
      "Responsibilities:\n",
      "Utilize statistical techniques and data visualization tools to analyze datasets related to product usage, customer behavior, and performance metrics.\n",
      "Develop and maintain key performance indicators (KPIs) and dashboards to monitor product performance, identify trends, and generate actionable insights for stakeholders.\n",
      "Collaborate with the data team, product managers and engineers to define and implement tracking mechanisms and data collection strategies to ensure accurate and reliable data collection.\n",
      "Conduct ad-hoc analysis to support strategic decision-making and provide recommendations to cross-functional teams.\n",
      "Design, build, and maintain scalable data pipelines and ETL processes to support our analytics and reporting needs.\n",
      "Collaborate with the development team to track and resolve data quality issues or discrepancies.\n",
      "Implement best practices for data governance, security, and compliance.\n",
      "Qualifications:\n",
      "Proven experience (2+ years) working as a Data Analyst, within the product, health, or technology areas.\n",
      "Proven experience (1+ years) working as a Data Engineer or similar role.\n",
      "Strong proficiency in SQL.\n",
      "Expertise in data visualization tools such as Google Analytics, Looker, Tableau, Power BI, or similar.\n",
      "Hands-on experience with cloud platforms such as GCP, AWS, or Azure.\n",
      "Proficiency in Python for scripting, automation, and data processing.\n",
      "Experience with workflow management tools such as Airflow for orchestrating data pipelines.\n",
      "Strong analytical and problem-solving skills, with a keen attention to detail.\n",
      "Excellent communication and presentation skills, with the ability to convey complex technical concepts to non-technical stakeholders.\n",
      "Proactive and self-motivated, with the ability to work independently and deliver results within tight deadlines.\n",
      "Experience working in an Agile development environment and collaborating with cross-functional teams is a plus.\n",
      "Knowledge of marketing platforms such as SFMC, Braze, Iterable, etc., for integrating and analyzing marketing data is a plus.\n",
      "Additional Information\n",
      "Willow takes a market-based approach to pay, and pay may vary depending on the successful candidate's location, job-related skills, experience, and qualifications. We regularly review and update our pay ranges and therefore, these ranges may be modified in the future.\n",
      "\n",
      "Compensation Range: $90K - $108K\n",
      "\n",
      "Benefits\n",
      "We offer our full-time employees comprehensive health benefits (medical, dental & vision plans, and other health and wellness plans), 401k plan, flexible time off, stock options, extended parental leave, as well as an annual performance bonus program (based on individual performance and company performance). Take a look at our Careers site to learn more about our benefits.\n",
      "Regarding our COVID-19 Vaccine Policy\n",
      "Our primary concern is for the health and well-being of our employees. Employees in roles that are expected to work in the office or travel will be required to provide proof of vaccination. Medical exemptions will be considered.\n",
      "\n",
      "Willow is proud to be an equal opportunity workplace. We are committed to equal employment opportunities regardless of gender, race, religion, sexual orientation, gender identity, age, marital status, disability, or Veteran status. See our Careers page for our commitment to diversity, equity and inclusion.\n",
      "----------------------------------------\n",
      "Job Title: Data Engineer III\n",
      "Company: Altais\n",
      "Location: Remote in Oakland, CA 94607\n",
      "Job Link: https://www.indeed.com/rc/clk?jk=4d0603113a36d1f1&bb=ZIMRsFk2_AHPe4UietgcKGVKCrwtRzImu_j8juB3GlaDTIliuGPTdrEM4cF-sKOgpX7wyNrpxvaplEn4z_OEQUNpHmzS3rVJ2XXi3O1ZeGd0WQB3Mx10KlBMx13Q2rZj&xkcb=SoCc67M3AbjOLl2M2p0NbzkdCdPP&fccid=65fb41effba8ae76&vjs=3\n",
      "Details: {}\n",
      "Description: Profile insights\n",
      "Find out how your skills align with the job description\n",
      "Skills\n",
      "Do you have experience in Visual Studio?\n",
      "Yes\n",
      "No\n",
      "Education\n",
      "Do you have a Bachelor's degree?\n",
      "Yes\n",
      "No\n",
      "Job details\n",
      "Here’s how the job details align with your profile\n",
      ".\n",
      "Pay\n",
      "$110,000 - $140,000 a year\n",
      "Job type\n",
      "Full-time\n",
      "Encouraged to apply\n",
      "Fair chance\n",
      "Benefits\n",
      "Pulled from the full job description\n",
      "Work from home\n",
      "Full job description\n",
      "About Our Company\n",
      "At Altais, we’re looking for bold and curious clinicians and innovators who share our passion for enabling better health care experiences and revolutionizing the healthcare system for physicians, advanced care providers, patients, and the clinical community. At Altais, we’re building breakthrough clinical support tools, technology, and services to let doctors do what they do best: care for people. We invite you to join our growing passionate team as we change the game for the future of healthcare and enable the experience that people need and deserve. Altais family of companies include Brown & Toland Physicians, Family Care Specialists and Altais Medical Groups.\n",
      "About Your Team\n",
      "Are you looking to work with a high performing, fast growing and dynamic IT team? Altais and our subsidiaries, form one of the most recognized medical groups in California. We are 4,000+ physicians, working in over 40 cities in California, caring for more than 500,000 patients. If working in a mission driven organization supporting highly competent, hard-working, thoughtful clinicians who value good ideas and are passionate about reshaping healthcare excites you, then we are thrilled to welcome you to your new career.\n",
      "About Your Work\n",
      "The Data Engineer works closely with Healthcare Analytics staff to develop system-based solutions and reports that help support clinical, financial and operational business processes and objectives. The incumbent demonstrates high level data warehouse understanding and is responsible for the creation and support of ETL, data marts, databases, reports and complex SQL. The Data Engineer will work with customers, employees and vendors to design and develop data warehouse structures that support reporting and analytical needs.\n",
      "You will focus on:\n",
      "Design, develop, and maintain robust data pipelines to support healthcare analytics initiatives.\n",
      "Collaborate with cross-functional teams to understand data requirements and ensure data availability for analysis.\n",
      "Implement data integration solutions for diverse healthcare data sources, ensuring data accuracy and reliability.\n",
      "Optimize and tune existing data infrastructure to improve performance and efficiency.\n",
      "Work closely with data scientists and analysts to translate business requirements into actionable data engineering tasks.\n",
      "Ensure compliance with healthcare data privacy and security regulations in all data processing activities.\n",
      "Troubleshoot data-related issues and provide timely resolutions to maintain data integrity.\n",
      "Explore and adopt emerging technologies to enhance data engineering capabilities and stay current with industry best practices.\n",
      "Create and maintain documentation for data engineering processes, pipelines, and workflows.\n",
      "Participate in the evaluation and selection of data management tools and technologies.\n",
      "The Skills, Experience & Education You Bring\n",
      "Solid understanding of database structures including the creation and support of complex views, stored procedures, and/or packages.\n",
      "High level of proficiency working with various data integration and visualization tools to include but not limited to, MS Excel, MS Access, MS SQL Server (SSIS), Talend, T-SQL, Power BI, MS Visual Studio.\n",
      "Skilled in reading and writing moderate to complex SQL commands and accurately assessing results (preferable 3+ years).\n",
      "Ability to analyze complex data structures with minimal direction.\n",
      "Automate data imports and extracts to key vendors and business partners.\n",
      "Strong aptitude for quickly troubleshooting and identifying the cause of questionable results within extracts and reports.\n",
      "Ability to efficiently research, design and build solutions to technical data challenges.\n",
      "Must be able to excel in a fast-moving team environment with a proven ability to take ownership over projects and complete them in a timely manner.\n",
      "High comfort level working on multiple simultaneous assignments and dealing with assignment prioritization changes.\n",
      "Excellent employee, client, and vendor relations a must.\n",
      "Strong facilitation, critical thinking, problem solving, decision making and analytical skills.\n",
      "Excellent verbal, written communication, and presentation skills.\n",
      "Self-starter who holds themself accountable for completing assignments timely and accurately.\n",
      "Experience & Education:\n",
      "Bachelor’s or higher degree in Computer Science, Information Technology, or a related field.\n",
      "5 years + proven experience in data engineering with a focus on healthcare data.\n",
      "Proficiency in programming languages such as SQL, Python, or Java for data processing tasks.\n",
      "Strong understanding of database systems, both relational and NoSQL, and experience with data modeling.\n",
      "Expertise in ETL (Extract, Transform, Load) processes and tools (SSIS and Talend)\n",
      "Familiarity with healthcare data standards, such as HL7, FHIR, and experience working with electronic health records (EHR) data.\n",
      "Knowledge of data warehousing concepts and technologies.\n",
      "Excellent problem-solving skills and attention to detail.\n",
      "Effective communication skills to collaborate with interdisciplinary teams.\n",
      "Understanding of healthcare analytics and a commitment to maintaining the highest data quality standards.\n",
      "You Share Our Mission & Values:\n",
      "You are passionate about improving the healthcare experience and want to be part of the Altais mission.\n",
      "You are bold and curious- willing to take risks, try new things and be creative.\n",
      "You take pride in your work and are accountable for the quality of everything you do, holding yourself and others to a high standard.\n",
      "You are compassionate and are known as someone who demonstrates emotional intelligence, considers others when making decisions and always tries to do the right thing.\n",
      "You co-create, knowing that we can be better as a team than individuals. You work well with others, collaborating and valuing diversity of thought and perspective.\n",
      "You build trust with your colleagues and customers by demonstrating that you are someone who values honesty and transparency.\n",
      "Altais values the contribution each Team Member brings to our organization. Final determination of a successful candidate’s starting pay will vary based on several factors, including, but not limited to education and experience within the job or the industry. The pay scale listed for this position is generally for candidates that meet the specified qualifications and requirements listed on this job description. Additional pay may be determined for those candidates that exceed these specified qualifications and requirements. We provide a competitive compensation package that recognizes your experience, credentials, and education alongside a robust benefits program to meet your needs.\n",
      "The anticipated pay range for this role is listed in our salary posting for transparency but may vary based on factors including the candidate’s qualifications, skills, experience, and geographic location. This base pay range is specific to California and can vary based on Bay Area, Metro LA, and Greater California regions which may not be applicable to other locations.\n",
      "Altais and its subsidiaries and affiliates are committed to protecting the privacy and security of the personal information you provide to us. Please refer to our ‘CPRA Privacy Notice for California Employees and Applicants’ to learn how we collect and process your personal information when you apply for a role with us.\n",
      "Physical Requirements: Office Environment - roles involving part to full time schedule in Office Environment. Based in our physical offices and work from home office/deskwork – Activity level: Sedentary, frequency most of workday.\n",
      "External hires must pass a background check/drug screen. Qualified applicants with arrest records and/or conviction records will be considered for employment in a manner consistent with Federal, State and local laws, including but not limited to the San Francisco Fair Chance Ordinance. All qualified applicants will receive consideration for employment without regards to race, color, religion, sex, national origin, sexual orientation, gender identity, protected veteran status or disability status and any other classification protected by Federal, State and local laws.\n",
      "----------------------------------------\n",
      "Job Title: Data Engineer\n",
      "Company: Obayashi Corporation\n",
      "Location: Foster City, CA 94404\n",
      "Job Link: https://www.indeed.com/rc/clk?jk=f63a707c5b25b6df&bb=ZIMRsFk2_AHPe4UietgcKEJ1kX6STmPpVwr1UhMEnhj8FJLVZSdYeu1xLs_j2Cyz_Why9k8O0lByWvQUMKK1pOyfmshNAlfslpgn1QGAQgLs4Vogn5AiMBPtW1monG5s&xkcb=SoAo67M3AbjOLl2M2p0MbzkdCdPP&fccid=d3671b3f677ac358&vjs=3\n",
      "Details: {}\n",
      "Description: Profile insights\n",
      "Find out how your skills align with the job description\n",
      "Skills\n",
      "Do you have experience in Snowflake?\n",
      "Yes\n",
      "No\n",
      "Education\n",
      "Do you have a Bachelor's degree?\n",
      "Yes\n",
      "No\n",
      "Job details\n",
      "Here’s how the job details align with your profile\n",
      ".\n",
      "Pay\n",
      "$125,000 - $150,000 a year\n",
      "Job type\n",
      "Full-time\n",
      "Location\n",
      "Foster City, CA 94404\n",
      "Benefits\n",
      "Pulled from the full job description\n",
      "401(k) matching\n",
      "Dental insurance\n",
      "Employee assistance program\n",
      "Health insurance\n",
      "Life insurance\n",
      "Vision insurance\n",
      "Full job description\n",
      "Position: Data Engineer\n",
      "Reports to: VP, Information Technology\n",
      "Location: Foster City, CA\n",
      "Primary Role\n",
      "This individual will support business operations and conduct data analytics projects to provide information and insights to key stakeholders for decision-making or solution development. They will implement data standards and deploy automation tools to extract, synthesize, and validate data from different sources and transform data into useable metrics. Create data sets, monitor data quality, troubleshoot, and resolves database issues to ensure data integrity.\n",
      "Roles and Responsibilities\n",
      "Define and own overall product vision, strategy, and success metrics for key initiatives around data and reporting and work with business to build and release in a timely manner.\n",
      "Engage with our customers to learn their business needs and build insightful & delightful products leveraging data.\n",
      "Interact with APIs that allow our business to integrate the in-house ERP, BI and Web applications to consume data frequently at scale in a safe and secure manner.\n",
      "Work on in-house ERP with various business functions to seamlessly integrate the business data and build dashboards and reports.\n",
      "Collaborate with group companies on key initiatives.\n",
      "Establish and cultivate strong relationships with senior executives across various businesses.\n",
      "Required Qualifications\n",
      "BS/BA degree with 5+ years of relevant experience\n",
      "Extensive knowledge and experience in Microsoft Power BI\n",
      "Proficient in SQL, Python and Data Modeling\n",
      "Experience on Snowflake/Big Data\n",
      "Experience on using APIs for integration with various systems\n",
      "Experience on managing and implementing Data Governance and security\n",
      "Excellent analytical and problem-solving skills\n",
      "Knowledge of Agile Methodology\n",
      "Ability to handle multiple projects and tasks simultaneously.\n",
      "About Obayashi\n",
      "Obayashi Corporation has delivered quality solutions to infrastructure challenges across the globe for more than 130 years. Founded in 1892, Obayashi is the largest Japanese Contractor, consistently ranked in the Top 20 ENR Global Contractors and is the fourth largest International Contractor conducting business in the United States. Obayashi has been operating and completing projects in the United States since 1979. Obayashi provides a full range of general contractor services in both public works and private sectors including commercial office, residential, tunnels, highways, rail systems, bridges, artificial islands, airports and nuclear power plants. Other services include urban planning and architecture, real estate and property development, environmental, and waste services. The North American Operations consolidated revenue is over $3billion in the United States and Canada. For more information, please visit www.obayashi-na.com\n",
      "The salary range for this position is $125,000 to $150,000 annually. This represents what we reasonable expect to offer. The final salary offered to the successful candidate will be dependent on several factors, including but not limited to individual qualifications, skills, and experience.\n",
      "Obayashi offers Competitive Market Salaries and Benefits Package including Medical PPO Plan, Dental DPO Plan, vision, EAP, Life Insurance & Disability, and 401K including matching funds. Health insurance premiums are 100% paid by Obayashi for employees. Family insurance premiums are paid by Obayashi at 80%.\n",
      "Equal Opportunity Employer\n",
      "Obayashi Corporation is an equal opportunity employer and makes employment decisions on the basis of merit. We want to have the best available person in every job. Company policy prohibits unlawful discrimination based on race, color, creed, gender, religion, marital status, registered domestic partner status, age, national origin or ancestry, physical or mental disability, medical condition including genetic characteristics, sexual orientation, or any other consideration made unlawful by federal, state, or local laws. It also prohibits unlawful discrimination based on the perception that anyone has any of those characteristics or is associated with a person who has or is perceived as having any of those characteristics. The Company is committed to compliance with all applicable laws providing equal employment opportunities. This commitment applies to all persons involved in Company operations and prohibits unlawful discrimination by any employee of the Company, including supervisors and coworker.\n",
      "----------------------------------------\n",
      "Job Title: Data Engineer (Databricks)\n",
      "Company: EHUB GLOBAL\n",
      "Location: Santa Clara, CA 95050\n",
      "Job Link: https://www.indeed.com/rc/clk?jk=d7b732f905fdfd79&bb=ZIMRsFk2_AHPe4UietgcKEPjqImjzbVenJw9YQ7mkfMxTiuHytwE0g3Mc93BIKSUF0fz-cGowWAg_jWRZgqNSnca4MkqVhcVq2KMo-6X9sHhWu2gEHuKrLMsvOae4yXe&xkcb=SoDB67M3AbjOLl2M2p0DbzkdCdPP&fccid=5be7603de6743699&cmp=ehub-global&ti=Data+Engineer&vjs=3\n",
      "Details: {}\n",
      "Description: Profile insights\n",
      "Find out how your skills align with the job description\n",
      "Certifications\n",
      "Do you have a valid Databricks Certified Data Engineer Professional certification?\n",
      "Yes\n",
      "No\n",
      "Skills\n",
      "Do you have experience in Spark?\n",
      "Yes\n",
      "No\n",
      "Education\n",
      "Do you have a Bachelor's degree?\n",
      "Yes\n",
      "No\n",
      "Job details\n",
      "Here’s how the job details align with your profile\n",
      ".\n",
      "Pay\n",
      "From $62 an hour\n",
      "Job type\n",
      "Contract\n",
      "Shift and schedule\n",
      "8 hour shift\n",
      "Monday to Friday\n",
      "Location\n",
      "2003 Acacia Court, Santa Clara, CA 95050\n",
      "Benefits\n",
      "Pulled from the full job description\n",
      "401(k)\n",
      "Dental insurance\n",
      "Health insurance\n",
      "Full job description\n",
      "Job Title: Data Engineer (Data Bricks & Backend)\n",
      "Location: Santa Clara, CA (onsite)\n",
      "Duration:12+months\n",
      "11 YEARS\n",
      "Overview:\n",
      "Seeking a Data Engineer with expertise in DataBricks, strong SQL skills, and exposure to project management principles. The role involves designing, building, and optimizing data pipelines while also supporting project coordination and planning activities. Ideal for candidates with technical prowess and a collaborative mindset.\n",
      "Key Responsibilities:\n",
      "Develop and maintain data pipelines using DataBricks for efficient data processing.\n",
      "Utilize SQL to query, transform, and analyze data from various sources.\n",
      "Assist in project planning, scheduling, and resource allocation.\n",
      "Collaborate with cross-functional teams to ensure data engineering aligns with project objectives.\n",
      "Troubleshoot data pipeline issues and implement solutions for improved performance.\n",
      "Support data analysis initiatives by providing clean, curated datasets to analysts and data scientists.\n",
      "Qualifications:\n",
      "Proficiency in DataBricks for building and optimizing data pipelines.\n",
      "Strong SQL skills for data querying, transformation, and analysis.\n",
      "Exposure to project management methodologies and coordination.\n",
      "Bachelor's degree in Computer Science, Engineering, or related field (preferred).\n",
      "Excellent communication and teamwork skills.\n",
      "Preferred Skills:\n",
      "Experience with cloud platforms (e.g., AWS, Azure, GCP).\n",
      "Familiarity with big data frameworks (e.g., Apache Spark).\n",
      "Knowledge of data warehousing concepts and tools.\n",
      "Ability to adapt to evolving technology trends and project requirements.\n",
      "Job Type: Contract\n",
      "Pay: From $62.00 per hour\n",
      "Expected hours: 40 per week\n",
      "Benefits:\n",
      "401(k)\n",
      "Dental insurance\n",
      "Health insurance\n",
      "Compensation package:\n",
      "Hourly pay\n",
      "Experience level:\n",
      "11+ years\n",
      "Schedule:\n",
      "8 hour shift\n",
      "Monday to Friday\n",
      "Experience:\n",
      "DataBricks: 10 years (Required)\n",
      "SQL: 10 years (Required)\n",
      "Project management: 2 years (Required)\n",
      "Ability to Commute:\n",
      "Santa Clara, CA 95050 (Required)\n",
      "Ability to Relocate:\n",
      "Santa Clara, CA 95050: Relocate before starting work (Required)\n",
      "Work Location: In person\n",
      "----------------------------------------\n",
      "Job Title: Data Engineer\n",
      "Company: Adobe\n",
      "Location: San Jose, CA 95110 \n",
      "(Downtown area)\n",
      "Job Link: https://www.indeed.com/rc/clk?jk=b0d406f04980ecba&bb=ZIMRsFk2_AHPe4UietgcKC48PYW-wUssX9XD-Mj67428GO8zqsZ1XHXxkaHeq2B0H1u-Aw5ZtA82X2Uqv0b_I0HwHqkASfBU1yWmI_pF3Ef3toEziZI9dQ%3D%3D&xkcb=SoB167M3AbjOLl2M2p0CbzkdCdPP&fccid=f89deb5a97c7738a&vjs=3\n",
      "Details: {}\n",
      "Description: Profile insights\n",
      "Find out how your skills align with the job description\n",
      "Skills\n",
      "Do you have experience in UNIX?\n",
      "Yes\n",
      "No\n",
      "Education\n",
      "Do you have a Bachelor's degree?\n",
      "Yes\n",
      "No\n",
      "Job details\n",
      "Here’s how the job details align with your profile\n",
      ".\n",
      "Pay\n",
      "$108,000 - $198,500 a year\n",
      "Job type\n",
      "Full-time\n",
      "Location\n",
      "San Jose, CA 95110\n",
      "Full job description\n",
      "Our Company\n",
      "\n",
      "Changing the world through digital experiences is what Adobe’s all about. We give everyone—from emerging artists to global brands—everything they need to design and deliver exceptional digital experiences! We’re passionate about empowering people to create beautiful and powerful images, videos, and apps, and transform how companies interact with customers across every screen.\n",
      "\n",
      "We’re on a mission to hire the very best and are committed to creating exceptional employee experiences where everyone is respected and has access to equal opportunity. We realize that new ideas can come from everywhere in the organization, and we know the next big idea could be yours!\n",
      "\n",
      "Adobe is seeking a Data Engineer who consistently demonstrates talent and motivation to join our exceptional Marketing Operations and Engineering department in San Jose. The Marketing Operations and Engineering team is part of Growth Marketing and Insights (GMI) organization and is responsible for the data architecture, transformation, and engineering needs of the entire organization. This includes managing backend data for various marketing measurement tools and processes, including attribution, product and customer segmentation, lifetime value, and more.\n",
      "\n",
      "Responsibilities:\n",
      "Design, develop, and maintain scalable and reliable data pipelines and infrastructure.\n",
      "Collaborate with teams from different departments to gather and analyze data requirements.\n",
      "Optimize data storage, processing, and retrieval for efficient and flawless operation.\n",
      "Implement data models and schemas to support various data-driven applications.\n",
      "Ensure data quality, integrity, and security throughout the data lifecycle.\n",
      "Perform data analysis and provide insights to drive informed decision-making.\n",
      "Keep yourself informed about the latest developments and effective approaches in data engineering.\n",
      "\n",
      "Requirements:\n",
      "Bachelor's degree in Computer Science, Engineering, or a related field, with at least 3 years of data engineering experience.\n",
      "Proven experience in designing and developing data pipelines and ETL processes.\n",
      "Strong programming skills in Python, Java, or Scala.\n",
      "Experience with Spark, Databricks, Airflow and related Big Data technologies\n",
      "Familiarity with cloud-based data platforms (e.g., AWS, Azure, GCP).\n",
      "Knowledge of SQL and database systems.\n",
      "Knowledge of UNIX and shell scripting\n",
      "Strong problem-solving and analytical abilities.\n",
      "Excellent communication and collaboration skills.\n",
      "Experience in marketing domain is a plus\n",
      "\n",
      "Join Adobe, a company that values diversity, inclusion, and equal opportunity. We are an equal opportunity employer and do not discriminate based on gender, race, color, ethnicity, national origin, age, disability, religion, sexual orientation, gender identity or expression, veteran status, or any other applicable characteristics protected by law. At Adobe, we believe that diverse perspectives drive innovation and creativity.\n",
      "\n",
      "Note: If you have a disability or special need that requires accommodation during the application process, please contact us at accommodations@adobe.com or call (408) 536-3015. We aim to ensure our website and application process are accessible to all users.\n",
      "\n",
      "We strictly adhere to fair competition practices and maintain a free and open marketplace for all employees. We have policies in place to ensure that we do not enter into illegal agreements with other companies to not recruit or hire each other's employees.\n",
      "Our compensation reflects the cost of labor across several U.S. geographic markets, and we pay differently based on those defined markets. The U.S. pay range for this position is $108,000 -- $198,500 annually. Pay within this range varies by work location and may also depend on job-related knowledge, skills, and experience. Your recruiter can share more about the specific salary range for the job location during the hiring process.\n",
      "\n",
      "At Adobe, for sales roles starting salaries are expressed as total target compensation (TTC = base + commission), and short-term incentives are in the form of sales commission plans. Non-sales roles starting salaries are expressed as base salary and short-term incentives are in the form of the Annual Incentive Plan (AIP).\n",
      "\n",
      "In addition, certain roles may be eligible for long-term incentives in the form of a new hire equity award.\n",
      "\n",
      "Adobe is proud to be an Equal Employment Opportunity and affirmative action employer. We do not discriminate based on gender, race or color, ethnicity or national origin, age, disability, religion, sexual orientation, gender identity or expression, veteran status, or any other applicable characteristics protected by law. Learn more.\n",
      "\n",
      "Adobe aims to make Adobe.com accessible to any and all users. If you have a disability or special need that requires accommodation to navigate our website or complete the application process, email accommodations@adobe.com or call (408) 536-3015.\n",
      "\n",
      "Adobe values a free and open marketplace for all employees and has policies in place to ensure that we do not enter into illegal agreements with other companies to not recruit or hire each other’s employees.\n",
      "----------------------------------------\n",
      "Job Title: Cloud Data Platform Engineer\n",
      "Company: Apple\n",
      "Location: Cupertino, CA\n",
      "Job Link: https://www.indeed.com/rc/clk?jk=f5009c786a033083&bb=ZIMRsFk2_AHPe4UietgcKEYH1pBQwQzd4WtNi8pK13yxXvoPuYvZdqMQC1Hgfhe2kl01JaBi7C8VJkTyVYXZC6Shl5tVduLYUSWJ-2vRGKIBXfRduw5bjA%3D%3D&xkcb=SoDo67M3AbjOLl2M2p0BbzkdCdPP&fccid=c1099851e9794854&vjs=3\n",
      "Details: {}\n",
      "Description: Profile insights\n",
      "Find out how your skills align with the job description\n",
      "Skills\n",
      "Do you have experience in Snowflake?\n",
      "Yes\n",
      "No\n",
      "Education\n",
      "Do you have a Bachelor's degree?\n",
      "Yes\n",
      "No\n",
      "Job details\n",
      "Here’s how the job details align with your profile\n",
      ".\n",
      "Pay\n",
      "$138,900 - $256,500 a year\n",
      "Job type\n",
      "Full-time\n",
      "Location\n",
      "Cupertino, CA\n",
      "Benefits\n",
      "Pulled from the full job description\n",
      "Dental insurance\n",
      "Employee stock purchase plan\n",
      "Health insurance\n",
      "RSU\n",
      "Retirement plan\n",
      "Full job description\n",
      "Summary\n",
      "\n",
      "Posted: May 20, 2024\n",
      "\n",
      "Weekly Hours: 40\n",
      "\n",
      "Role Number:200508564\n",
      "\n",
      "The people here at Apple don’t just build products - we craft the kind of wonder that’s revolutionized entire industries. It’s the diversity of those people and their ideas that supports the innovation that runs through everything we do, from amazing technology to industry-leading environmental efforts. Join Apple, and help us leave the world better than we found it. Would you like to be a part of one of the world’s fastest growing data warehouses for the world’s largest company? Does the prospect of being challenged with sophisticated database design and optimization problems excite you? The Global Business Intelligence (GBI) team within Apple's IS&T organization has a very large Enterprise Data Warehouse (EDW) to support analytical and reporting needs of hundreds of global Apple users. This is an extraordinary opportunity for an inquisitive, experienced and results-oriented database expert to work on our EDW platform to provide a scalable, high-performance and active data-warehousing platform. You are self-driven, highly motivated, innovative, have a good work ethic and have a consistent track record of designing and leading large, complex data warehouses. The position will involve close interaction with development, product, database administrators and support teams. This person will review database designs to ensure that they are optimized for performance, scalability and be a thought leader and champion in automation. This role will provide second-level support to address any platform wide performance and stability issues.\n",
      "\n",
      "Description\n",
      "\n",
      "Participation in Physical Model reviews with the development teams to provide inputs on improving performance Continuous analysis of the database environment to capitalize on system-wide tuning opportunities Act as an inspiring leader and conduct brown bag lunch sessions for development teams to create awareness regarding the value proposition of new and upcoming features Performance optimization of database resources intensive ETL and individual user queries Drive new features testing, rollout and `adoption Workload Management analysis and improvement to meet response-time SLA's as well as optimize resource consumption Influence, establish best engineering practices through solid design decisions, processes and automation\n",
      "\n",
      "Minimum Qualifications\n",
      "\n",
      "Minimum Qualifications\n",
      "\n",
      "Key Qualifications\n",
      "3+ years experience in relational cloud based database technologies like Snowflake\n",
      "Ability to identify system level bottlenecks in the database and recommend solutions\n",
      "Object oriented programming experience in Python/Java or Scala\n",
      "Analyze production workloads and develop strategies to run Snowflake database with scale and efficiency\n",
      "Experience in Snowflake performance tuning, capacity planning, managing cloud spend and utilization is a plus\n",
      "Manage all aspects of database security both at infrastructure and application level\n",
      "Ability to articulate and enforce Snowflake best practices and find ingenious way of enforcing them\n",
      "Manage Cross regional data replication and be an expert at database recovery features\n",
      "Ability to define and automate DBA functions\n",
      "Experience with AWS or GCP or Azure cloud specifically in compute , storage and security services\n",
      "\n",
      "Preferred Qualifications\n",
      "\n",
      "Preferred Qualifications\n",
      "\n",
      "Education & Experience\n",
      "\n",
      "BS in Computer Science or equivalent\n",
      "\n",
      "Additional Requirements\n",
      "\n",
      "Pay & Benefits\n",
      "At Apple, base pay is one part of our total compensation package and is determined within a range. This provides the opportunity to progress as you grow and develop within a role. The base pay range for this role is between $138,900 and $256,500, and your base pay will depend on your skills, qualifications, experience, and location. Apple employees also have the opportunity to become an Apple shareholder through participation in Apple’s discretionary employee stock programs. Apple employees are eligible for discretionary restricted stock unit awards, and can purchase Apple stock at a discount if voluntarily participating in Apple’s Employee Stock Purchase Plan. You’ll also receive benefits including: Comprehensive medical and dental coverage, retirement benefits, a range of discounted products and free services, and for formal education related to advancing your career at Apple, reimbursement for certain educational expenses - including tuition. Additionally, this role might be eligible for discretionary bonuses or commission payments as well as relocation. Learn more about Apple Benefits. Note: Apple benefit, compensation and employee stock programs are subject to eligibility requirements and other terms of the applicable plan or program.\n",
      "\n",
      "More\n",
      "Apple is an equal opportunity employer that is committed to inclusion and diversity. We take affirmative action to ensure equal opportunity for all applicants without regard to race, color, religion, sex, sexual orientation, gender identity, national origin, disability, Veteran status, or other legally protected characteristics. Learn more about your EEO rights as an applicant.\n",
      "----------------------------------------\n",
      "Job Title: Data Engineer\n",
      "Company: Indium Software\n",
      "Location: Cupertino, CA 95014\n",
      "Job Link: https://www.indeed.com/rc/clk?jk=c7e09dd5fe2ecfa4&bb=ZIMRsFk2_AHPe4UietgcKOMCfqtupSPxnh3oCeDwJKedAJu-LTW1ZtGWBg4DM3MeLNp6gqUdyCeHHsUd5cR81Dj0uWIesEnHqolNCqiy27M97ppcJ74_XpTPtfhUN_L-&xkcb=SoBc67M3AbjOLl2M2p0AbzkdCdPP&fccid=2c25ea6cf84dad59&vjs=3\n",
      "Details: {}\n",
      "Description: Profile insights\n",
      "Find out how your skills align with the job description\n",
      "Skills\n",
      "Do you have experience in Oracle?\n",
      "Yes\n",
      "No\n",
      "Job details\n",
      "Here’s how the job details align with your profile\n",
      ".\n",
      "Job type\n",
      "Contract\n",
      "Location\n",
      "Cupertino, CA 95014\n",
      "Full job description\n",
      "Job Information\n",
      "RSD NO\n",
      "8909\n",
      "Industry\n",
      "IT Services\n",
      "Min Experience\n",
      "3\n",
      "Max Experience\n",
      "5\n",
      "City\n",
      "Cupertino\n",
      "State/Province\n",
      "California\n",
      "Country\n",
      "United States\n",
      "Zip/Postal Code\n",
      "95014\n",
      "Job Description\n",
      "Title: Data Modeler\n",
      "\n",
      "Location: Atlanta, GA\n",
      "\n",
      "Duration: Long Term Contract / Permanent\n",
      "\n",
      "Work Mode: Work from Office\n",
      "\n",
      "Responsibilities\n",
      "\n",
      "Develop, maintain, and enhance conceptual, logical, and physical data models for various business domains and applications.\n",
      "Experience in building Data models in OLTP Systems from the scratch.\n",
      "Experience in building generic DATA models for File Processing, Document Processing,\n",
      "Hierarchy models, Data Auditing, Notifications.\n",
      "Translate business requirements into data models ensuring structural integrity and compliance with organizational standards.\n",
      "Design and optimize database structures to support data storage, retrieval, and analysis.\n",
      "Collaborate with database administrators and developers to implement data models in database systems (SQL Server, Oracle).\n",
      "Ensure data integrity, security, and performance through proper database design and optimization techniques.\n",
      "Perform data profiling, analysis, and validation to identify inconsistencies and anomalies.\n",
      "Collaborate with stakeholders to elicit, analyze, and document data requirements.\n",
      "Create clear and comprehensive documentation of data models, including entity-relationship diagrams, data dictionaries, and metadata.\n",
      "Support data integration projects by mapping data sources to target data models and\n",
      "designing data transformation processes.\n",
      "Facilitate data migration efforts, ensuring seamless transition of data from legacy systems to new platforms or environments.\n",
      "Work closely with business analysts, developers, and project managers to align data modelling efforts with project timelines and objectives.\n",
      "Conduct quality assurance reviews of data models to ensure accuracy, completeness, and usability.\n",
      "Identify and resolve data model-related issues, such as performance bottlenecks, data anomalies, and schema conflicts.\n",
      "Contribute to the overall success of the team by actively participating in meetings, sharing insights, and contributing innovative ideas.\n",
      "\n",
      "Indium Software is an Equal Opportunity Employer and does not discriminate on the basis of race or ethnicity, religion, sex, national origin, age, veteran disability or genetic information or any other reason prohibited by law in employment.\n",
      "----------------------------------------\n",
      "Job Title: Data Engineer\n",
      "Company: LatentView Analytics\n",
      "Location: San Jose, CA\n",
      "Job Link: https://www.indeed.com/rc/clk?jk=1c2d56f3eae17069&bb=ZIMRsFk2_AHPe4UietgcKGVd3RJNAqK577yB2GhEUvNVtyWX-3SZ7PUkxfkkZePIYBvFVDFghc8oyoEsltxF2gg3XT7qXz2xg82E1aAiY4T9ioIPfFykF4KhaagU4Z3J&xkcb=SoDS67M3AbjOLl2M2p0HbzkdCdPP&fccid=20a24f005983fed4&vjs=3\n",
      "Details: {}\n",
      "Description: Profile insights\n",
      "Find out how your skills align with the job description\n",
      "Skills\n",
      "Do you have experience in SQL?\n",
      "Yes\n",
      "No\n",
      "Job details\n",
      "Here’s how the job details align with your profile\n",
      ".\n",
      "Job type\n",
      "Permanent\n",
      "Location\n",
      "San Jose, CA\n",
      "Full job description\n",
      "San Jose, California, United States\n",
      "Department\n",
      "BET\n",
      "Job posted on\n",
      "May 04, 2024\n",
      "Employment type\n",
      "Permanent\n",
      "\n",
      "Description:\n",
      "We are seeking a skilled and experienced Data Engineer to join our dynamic team. As a Data Engineer, you will play a crucial role in designing, implementing, and maintaining data pipelines and systems that enable efficient data processing and analysis.\n",
      "Collaborate with cross-functional teams to understand business requirements and translate them into scalable data solutions. The ideal candidate will have expertise in project management, client management, SQL, Azure services, ETL pipelines, data validation, and a passion for leveraging data to drive business insights.\n",
      "\n",
      "Responsibilities:\n",
      "Project Management: Lead data engineering projects from inception to completion, ensuring timely delivery and alignment with business objectives. Coordinate with stakeholders to gather requirements, define project scope, and manage resources effectively.\n",
      "Client Management: Serve as the primary point of contact for clients, understanding their data needs, and providing technical guidance and support. Build and maintain strong client relationships through effective communication and responsiveness.\n",
      "SQL Expertise: Design and optimize complex SQL queries for data extraction, transformation, and loading (ETL). Perform advanced data manipulation and analysis to derive actionable insights from large datasets.\n",
      "Azure Knowledge: Demonstrate proficiency in Azure cloud services, including Azure Data Factory, Azure SQL Database, Azure Databricks, and Azure Storage. Architect and implement scalable data solutions on the Azure platform, adhering to best practices and security standards.\n",
      "ETL Pipelines: Develop robust ETL pipelines to automate data ingestion, cleansing, and transformation processes. Implement data integration solutions to facilitate seamless data flow across various systems and applications.\n",
      "Data Validation: Design and implement data validation strategies to ensure data accuracy, consistency, and integrity. Develop data quality checks and monitoring mechanisms to proactively identify and resolve data issues.\n",
      "Collaboration: Collaborate with data scientists, analysts, and other stakeholders to understand data requirements and deliver actionable insights. Work closely with cross-functional teams to integrate data solutions into business processes and applications.\n",
      "\n",
      "Skills:\n",
      "Data Engineering, Azure, ETL PIpelines, SQL\n",
      "\n",
      "At LatentView Analytics, we value a diverse, inclusive workforce and we provide equal employment opportunities for all applicants and employees. All qualified applicants for employment will be considered without regard to an individual’s race, color, sex, gender identity, gender expression, religion, age, national origin or ancestry, citizenship, physical or mental disability, medical condition, family care status, marital status, domestic partner status, sexual orientation, genetic information, military or veteran status, or any other basis protected by federal, state or local laws.\n",
      "----------------------------------------\n",
      "Job Title: Data Engineer II\n",
      "Company: GreatSchools\n",
      "Location: Remote in Oakland, CA\n",
      "Job Link: https://www.indeed.com/rc/clk?jk=b5fef9c2c264948a&bb=ZIMRsFk2_AHPe4UietgcKBxPYwVMuTJq_UtCvl9JcYF4GXx5zNcCuPmVy6C_ecDizM376kHNjX0EcgNRBAeisDDXruuz35U8rem9hGwgZZ20HHuHH9B7F2oH8sLuJg3v&xkcb=SoBm67M3AbjOLl2M2p0GbzkdCdPP&fccid=0fda5022ae5c085a&vjs=3\n",
      "Details: {}\n",
      "Description: Profile insights\n",
      "Find out how your skills align with the job description\n",
      "Skills\n",
      "Do you have experience in Spark?\n",
      "Yes\n",
      "No\n",
      "Job details\n",
      "Here’s how the job details align with your profile\n",
      ".\n",
      "Pay\n",
      "$100,000 - $110,000 a year\n",
      "Job type\n",
      "Full-time\n",
      "Benefits\n",
      "Pulled from the full job description\n",
      "Dental insurance\n",
      "Health insurance\n",
      "Health savings account\n",
      "Retirement plan\n",
      "Vision insurance\n",
      "Full job description\n",
      "Applicants must be authorized to work for ANY employer in the U.S. We are unable to sponsor or take over sponsorship of an employment visa at this time.\n",
      "\n",
      "About The Opportunity\n",
      "GreatSchools is seeking an experienced Data Engineer II to join our Data Engineering Team in delivering the data and tools that allow GreatSchools to provide the most up-to-date, comprehensive, and informative lens on school quality possible. If you love working with data and want to use your talents to help tens of thousands of parents everyday find the right school for their child, then this position is for you.\n",
      "\n",
      "This is a fully remote position that can be based anywhere in the United States, requiring travel several times a year. This position reports to the Manager of our Data Engineering team.\n",
      "\n",
      "About GreatSchools\n",
      "GreatSchools is the leading nonprofit providing high-quality information that supports parents pursuing a great education for their child, schools striving for excellence, and communities working to diminish inequities in education. We are the only national organization that collects and analyzes data from all 51 state departments of education and the federal government to provide analysis, insights, and school quality ratings for parents, partners, researchers, and policymakers. Over 45 million users visit GreatSchools’ award-winning website annually to learn about schools in their area, explore research insights, and access thousands of free, evidence-based parenting resources to support their child’s learning and well-being. We are a mission-driven team that believes all children — especially those who have been historically underserved by the education system — deserve an excellent education.\n",
      "\n",
      "Key Responsibilities\n",
      "Contribution to and maintenance of preprocessing and loading processes, tooling, and documentation. Including shared Python libraries, automation services, and other key infrastructure.\n",
      "Contribution to loading efforts of K-12 education data into our data warehouse\n",
      "Contribution to improvements of the data warehouse architecture\n",
      "Design and Development of new ETL and data governance tools\n",
      "Contribution to the GreatSchools process and methodology documentation\n",
      "Performance of QA checks and/or pull request reviews of code submitted by other Data Engineers, using and building upon QA automation efforts/tools.\n",
      "Ability to perform size and difficulty assessment for acquired data loads\n",
      "Support of work to answer data-related internal and external stakeholder requests, including technical support on data related issues from our Customer Service team\n",
      "The responsibilities of this role may include a number of other similar or related duties which may not be specifically included within this position description, but which are consistent with the general level of the job.\n",
      "\n",
      "Qualifications\n",
      "You’ll likely find success in this role if you have:\n",
      "3-5 years of experience as a Data Engineer or in a similar role\n",
      "3-5 years of experience with SQL (including complex query writing, and storage and query performance tuning)\n",
      "3-5 years of experience with a programming language (python preferred)\n",
      "Experience with git (or another version control system or feedback-facilitator)\n",
      "Experience building large-scale projects from conception to completion\n",
      "The ability to apply a detail-oriented, critical eye to data quality\n",
      "Strong organizational, problem-solving, and analytical skills.\n",
      "Experience working with cross-functional teams in a dynamic, remote environment\n",
      "Self-starter; with the ability to work independently and as a strong contributor to team projects\n",
      "Commitment to GreatSchools' mission and values; ability and excitement to work with a diverse group of people.\n",
      "Nice to haves:\n",
      "Experience with relational database management\n",
      "Experience with data processing and workflow management tools (such as Airflow, Spark, Luigi, Azkaban, etc.)\n",
      "Experience working with AWS data technologies (such as S3, Glue, Lambda,Redshift, etc.)\n",
      "Experience working with K-12 education data and/or working for non-profits\n",
      "\n",
      "Location\n",
      "Anywhere in the United States\n",
      "Travel up to 3 to 4 times annually primarily to the San Francisco Bay Area\n",
      "\n",
      "Salary & Benefits\n",
      "Base compensation for this role ranges from $100,000 to $110,000 for candidates residing in the Bay Area; salaries are dependent upon candidate location and experience.\n",
      "\n",
      "Our wide-ranging benefits are some of the best you’ll find out there. They include:\n",
      "Comprehensive medical, dental, and vision benefits with range of PPO, HMO, HSA-eligible accounts,\n",
      "Retirement plan with employer match;\n",
      "15 days of vacation, 9 days of sick leave, and 13 paid holidays annually; and\n",
      "Every five years, employees can rest and recharge with a paid four-week sabbatical.\n",
      "\n",
      "GreatSchools team members are diverse in all ways. We are committed to hiring talented staff who reflect the diversity of the communities and audiences we serve and who believe in supporting all parents, especially those who have been historically underserved. As a proud Equal Opportunity Employer, we are committed to considering applicants regardless of race, color, ancestry, national origin, religion, creed, age, disability (mental and physical), sex, gender (including pregnancy, childbirth, breastfeeding or related medical conditions) sexual orientation, gender identity, gender expression, medical condition, genetic information, marital status, military or veteran status, or any other federal, state or local protected class.\n",
      "\n",
      "Employment Fraud Notice: GreatSchools cautions job seekers of potential scammers attempting to impersonate our organization during the hiring process. All communications related to application submissions, interviews, and applicant selection are sent via our secure hiring portal, BambooHR. We will never ask you for financial information or payment and do not utilize any third-party recruiting services. You can verify our current employees here. Data privacy and security are critically important to us, so please note we are taking all available precautions to avoid fraudulent activity. Any questions or suspicious activity can be reported to hr@greatschools.org.\n",
      "----------------------------------------\n",
      "Job Title: Data Engineer\n",
      "Company: Zoom Video Communications, Inc.\n",
      "Location: Hybrid work in San Jose, CA 95113\n",
      "Job Link: https://www.indeed.com/rc/clk?jk=d75240335636889e&bb=ZIMRsFk2_AHPe4UietgcKFwNGJbEBamBHAuDNyhJOAJ5yCVWzIYMjCChKmvemlYlu7zrxmNA4dkFruZrXt4_dR0i2jqc6lVtvzpfE-7W9CSvYYjvyzmhS40zqsmXx7WH&xkcb=SoD767M3AbjOLl2M2p0FbzkdCdPP&fccid=e32d933c26e873c8&vjs=3\n",
      "Details: {}\n",
      "Description: Profile insights\n",
      "Find out how your skills align with the job description\n",
      "Skills\n",
      "Do you have experience in Scala?\n",
      "Yes\n",
      "No\n",
      "Education\n",
      "Do you have a Bachelor's degree?\n",
      "Yes\n",
      "No\n",
      "Job details\n",
      "Here’s how the job details align with your profile\n",
      ".\n",
      "Pay\n",
      "$232,690 a year\n",
      "Job type\n",
      "Full-time\n",
      "Encouraged to apply\n",
      "Fair chance\n",
      "Benefits\n",
      "Pulled from the full job description\n",
      "Opportunities for advancement\n",
      "Full job description\n",
      "Job Description:\n",
      "Partner with architecture and other senior leads to address the data needs of our rapidly-growing business; partner with data scientists, sales, marketing, operation, and product teams to build and deploy machine learning models that unlock growth; build custom integrations between cloud-based systems using APIs; write complex and efficient code to transform raw data sources into easily accessible models by coding across several languages such as Java, Python, Scala and/or SQL; and design, build, and launch new data models that provide intuitive analytics to the team.\n",
      "Minimum Education & Experience Requirements:\n",
      "Requires a Bachelor’s degree in Computer Science, Information Systems, Data Science, a related field, or a foreign equivalent. Must have 4 years of experience in job offered or related occupation. Must have 4 years of experience in SQL and NoSQL Databases; Programming language Python / Java / Scala; Data orchestration and data transformation tools; Devops and SCM (Git); Agile planning, estimation, delivery; and enterprise data warehouse and Visualization Tools/Frameworks. Telecommuting work arrangement permitted: position may work in various unanticipated locations throughout the U.S.\n",
      "Company Name: Zoom Video Communications, Inc.\n",
      "#LI-DNI\n",
      "Salary Range or On Target Earnings:\n",
      "Minimum:\n",
      "$232,690.00\n",
      "Maximum:\n",
      "$232,690.00\n",
      "In addition to the base salary and/or OTE listed Zoom has a Total Direct Compensation philosophy that takes into consideration; base salary, bonus and equity value.\n",
      "\n",
      "Note: Starting pay will be based on a number of factors and commensurate with qualifications & experience.\n",
      "We also have a location based compensation structure; there may be a different range for candidates in this and other locations.\n",
      "Ways of Working\n",
      "Our structured hybrid approach is centered around our offices and remote work environments. The work style of each role, Hybrid, Remote, or In-Person is indicated in the job description/posting.\n",
      "Benefits\n",
      "As part of our award-winning workplace culture and commitment to delivering happiness, our benefits program offers a variety of perks, benefits, and options to help employees maintain their physical, mental, emotional, and financial health; support work-life balance; and contribute to their community in meaningful ways. Click Learn for more information.\n",
      "About Us\n",
      "Zoomies help people stay connected so they can get more done together. We set out to build the best collaboration platform for the enterprise, and today help people communicate better with products like Zoom Contact Center, Zoom Phone, Zoom Events, Zoom Apps, Zoom Rooms, and Zoom Webinars.\n",
      "We’re problem-solvers, working at a fast pace to design solutions with our customers and users in mind. Here, you’ll work across teams to deliver impactful projects that are changing the way people communicate and enjoy opportunities to advance your career in a diverse, inclusive environment.\n",
      "\n",
      "Our Commitment\n",
      "We believe that the unique contributions of all Zoomies is the driver of our success. To make sure that our products and culture continue to incorporate everyone's perspectives and experience we never discriminate on the basis of race, religion, national origin, gender identity or expression, sexual orientation, age, or marital, veteran, or disability status. Zoom is proud to be an equal opportunity workplace and is an affirmative action employer. All your information will be kept confidential according to EEO guidelines.\n",
      "We welcome people of different backgrounds, experiences, abilities and perspectives including qualified applicants with arrest and conviction records and any qualified applicants requiring reasonable accommodations in accordance with the law. If you need any assistance or accommodations due to a medical condition, or if you need assistance accessing our website or completing the application process, please let us know by emailing us at careers@zoom.us.\n",
      "Good news – this job posting is more like a marathon, not a sprint, so it could be available for a while! We're on the lookout for awesome folks to join Zoom in various similar roles. No need to rush, just hit us up whenever you're ready to apply. We're always keeping an eye out for amazing talent!\n",
      "----------------------------------------\n",
      "Job Title: Azure Data Engineer\n",
      "Company: Lucayan Technology Solutions\n",
      "Location: Sunnyvale, CA\n",
      "Job Link: https://www.indeed.com/rc/clk?jk=2eba2ce5f2215623&bb=csTtYRd9oU4o9SJxQgVHM9bE_RNt51uRwnlhMMuYwM_40v8GOEmM2L1XrTVOwveMfHhrewnqsHkivRdN3GGe2fqq-u2Z5rKEad8QDMyrjau2pmfMWgdpwfLWRWm5-9tW&xkcb=SoCA67M3AbjI53ydd50LbzkdCdPP&fccid=8c3fad7f249a93f3&vjs=3\n",
      "Details: {}\n",
      "Description: Profile insights\n",
      "Find out how your skills align with the job description\n",
      "Skills\n",
      "Do you have experience in Usability?\n",
      "Yes\n",
      "No\n",
      "Education\n",
      "Do you have a Bachelor's degree?\n",
      "Yes\n",
      "No\n",
      "Location\n",
      "Sunnyvale, CA\n",
      "Full job description\n",
      "OUR COMPANY REVOLVES AROUND MISSION-DRIVEN ENGINEERING\n",
      "\n",
      "At Lucayan Solutions, we strive to solve our customer's hardest problems. Our highly focused customer-centric approach is crucial to our customer's success and ultimately ours. We aim to be a breath of fresh air: and be the most innovative organization in the Commercial contracting space. To get there, we need exceptionally talented, bright, and driven people. Join us if you'd like to be a part of our journey. Right here, right now, this is your chance to make history and put a ding in the universe.\n",
      "\n",
      "Lucayan is hiring an AZURE DATA ENGINEER to join our team. This position offers an exciting opportunity for someone with professional office experience who is eager to grow and learn.\n",
      "\n",
      "Data Bricks(Must Have)\n",
      "Strong hands on in Pyspark and Apache Spark\n",
      "Strong hands on in Medallion architecture\n",
      "Experience in Native Spark Migration to Databricks.\n",
      "Experience in Building Data Governance Solutions like Unity Catalog, Azure Purview etc.\n",
      "Highly experienced in Usability Optimization (Auto Compaction, Ordering, Vacuuming), Cost Optimization and Performance Optimization.\n",
      "Build Very Strong Orchestration Layer in Databricks/ADF…. Workflows.\n",
      "Build CICD for Databricks in Azure Devops.\n",
      "Process near Real time Data thru Auto Loader, DLT Pipelines.\n",
      "Implement Security Layer in Delta Lake.\n",
      "Implement Massive Parallel Processing Layers in Spark SQL and PySpark.\n",
      "Implement Cost effective Infrastructure in Databricks.\n",
      "Experience In extracting logic and from on prem layers, SAP, ADLS into Pyspark/ADLS using ADF/Databricks.\n",
      "\n",
      "Azure Synapse Analytics/Azure data Factory (ADF)(Must Have)\n",
      "Hands on Experience in Azure Synapse Analytics, Azure Data Factory and Data Bricks, Azure Storage, Azure Key Vault, SQL Pools CI/CD Pipeline Designing and other Azure services like functions, logic apps\n",
      "Linked services, Various Runtimes, Datasets, Pipelines, Activities\n",
      "Strong Hands-on Experience in Various Activities like Control flow logic and conditions (For Each, if, switch, until), Lookup, Stored procedure, scripts, validations, Copy Data, Data flow, Azure functions, Notebooks, SQL Pool Stored procedures and etc\n",
      "Strong hands on exp in deployment of code throughout landscape (Dev -> QA -> Prod), Git Hub, CI/CD pipelines and etc\n",
      "\n",
      "SQL Server stored procedures(Must have)\n",
      "strong hands on creating the SQL stored procedures\n",
      "Functions, Stored Procedures, how to call one SP into another, How to process record-by-record\n",
      "Dynamic SQL\n",
      "\n",
      "Python(Must have)\n",
      "Must have strong background about the Python libraries like PySpark, Pandas, NumPy, pymysql, Oracle, Pyspark libraries\n",
      "Must have strong hands on to get data through APIs\n",
      "Must be able to install libraries and help users to troubleshoot issues\n",
      "Must have knowledge to get the data through stored procedures via Python\n",
      "Should be able to debug the Python code\n",
      "\n",
      "Sparks(Must have)\n",
      "Hands on experience in Spark Pools, PySpark\n",
      "Should be able to merge data/delta loads through Notebooks\n",
      "Must have strong background about the Python libraries and PySpark\n",
      "\n",
      "Education: Bachelors Degree\n",
      "----------------------------------------\n",
      "Job Title: Data Engineer\n",
      "Company: Edmunds.com\n",
      "Location: Hybrid work in Santa Monica, CA 90404\n",
      "Job Link: https://www.indeed.com/rc/clk?jk=38324d1a5650f40f&bb=csTtYRd9oU4o9SJxQgVHM0DSO0rw7LltRf5SrcRTwNPydIZOxqPTbl-o7-vCEdA50cJWByBqgVWHs1t9-Lzh50I889qPm5ISEdgW9qkmrZyC47iYjt2F_A%3D%3D&xkcb=SoA067M3AbjI53ydd50KbzkdCdPP&fccid=f69fe5673875c177&vjs=3\n",
      "Details: {}\n",
      "Description: Profile insights\n",
      "Find out how your skills align with the job description\n",
      "Skills\n",
      "Do you have experience in Spark?\n",
      "Yes\n",
      "No\n",
      "Job details\n",
      "Here’s how the job details align with your profile\n",
      ".\n",
      "Pay\n",
      "$109,900 - $191,815 a year\n",
      "Job type\n",
      "Full-time\n",
      "Encouraged to apply\n",
      "Fair chance\n",
      "Benefits\n",
      "Pulled from the full job description\n",
      "401(k) matching\n",
      "Dental insurance\n",
      "Flexible spending account\n",
      "Health insurance\n",
      "Health savings account\n",
      "Paid parental leave\n",
      "Paid time off\n",
      "Show more\n",
      "Full job description\n",
      "Edmunds offers flexibility to work fully remote, from our Edquarters, or a combination of both\n",
      "\n",
      "At Edmunds we’re driven to make car buying easier. Ever since we began publishing printed car guides in the 60’s, the company has been in the business of trust, innovating ways to empower and support car shoppers. When Edmunds launched the car industry’s first Internet site in 1994, we established a leadership position online and have never looked back. Now, as one of the most trusted review sites on the Internet, millions of visitors use our research, shopping and buying tools every month to make an easy and informed decision on their next car. For consumers, we bring peace of mind. For dealers, we make tools to help them solve their problems and sell more cars. How do we do it, you ask? The key ingredients are our enthusiastic employees, progressive company culture and cutting-edge technology. Want to join the team? Read on to find out how!\n",
      "\n",
      "What You’re Applying For:\n",
      "Edmunds is looking for a Data Engineer to help us manage the data explosion dilemma! You will be joining a strong, determined and results-oriented Data Engineering team that is transforming Edmunds from IT to real time data-driven. You will get hands-on experience solving complex data problems using Big Data frameworks and methodologies. You’ll be helping the company make real time decisions based on the torrent of data it receives, empowering analytics on existing products as well as providing insights on potential new opportunities for growth.\n",
      "\n",
      "What You’ll Do:\n",
      "Create and maintain scalable, maintainable and reliable data pipelines that process very large quantities of structured and unstructured data in both batch and real time.\n",
      "Enhance and maintain the data lakehouse that powers the core of the company’s decision making process.\n",
      "Identify, design, and implement internal process improvements: automating manual processes, optimizing data delivery, re-designing infrastructure for greater scalability, etc.\n",
      "Collaborate with team members with a goal of improving personal knowledge of the systems, ensuring that code changes meet business goals and technology best practices.\n",
      "Regularly interact and collaborate with colleagues across functions and teams.\n",
      "Work with stakeholders including the Executive, Product, and Data teams to assist with data-related technical issues and support their data infrastructure needs.\n",
      "\n",
      "What You Need:\n",
      "Excellent problem solving, troubleshooting, and communication skills especially in a hybrid and remote environment.\n",
      "Desire to learn new technologies.\n",
      "Demonstrated ability to design and write maintainable software.\n",
      "Understanding of software engineering best practices, object oriented analysis & design, and design patterns & algorithms.\n",
      "Experience enhancing and evolving existing systems.\n",
      "Almost all of our codebase is in Scala and Python, but we only require that you have a high proficiency in at least one object oriented or functional programming language.\n",
      "A strong candidate will also have:\n",
      "Experience writing ETL Jobs and working with data at scale.\n",
      "Experience writing and maintaining real time / streaming data pipelines.\n",
      "Familiarity with some of the following: Spark, Scala, Python, AWS, Databricks, Airflow.\n",
      "Fluency in SQL\n",
      "Other nice to haves: Kubernetes, Machine Learning.\n",
      "\n",
      "The compensation range for this position is $109,900 - $191,815 per year. The base pay will take into account internal equity as well as job-related knowledge, skills, and experience among other factors. In addition, Edmunds offers full-time employees a comprehensive total rewards package including the benefits listed below.\n",
      "Edmunds Perks:\n",
      "Flexible time off\n",
      "13 Paid Holidays\n",
      "Comprehensive Health Benefits (medical, dental, vision, life and disability)\n",
      "Flexible Spending Accounts (Employees) and Health Savings Accounts (Employee and Employer Contributions)\n",
      "401K Plan with company matching at 100%, up to 6% of eligible salary with immediate vesting\n",
      "Stock purchase program\n",
      "CarMax vehicle discount\n",
      "Up to 4 months Paid Parental Leave\n",
      "HeartCash matches employee donations to the causes that are important to them\n",
      "2 Days of Paid Time Off for time to dedicate to social impact causes\n",
      "FitCash covers a portion of gym or fitness activity fees\n",
      "Well being sessions and events such as yoga, meditation and walking challenges\n",
      "On-going career development sessions and an annual learning event\n",
      "Pet insurance\n",
      "Sabbatical leave\n",
      "Education Reimbursement\n",
      "Pre-tax spending accounts for qualified transportation expenses\n",
      "Plus a coffee bar, frozen yogurt and more!\n",
      "\n",
      "Working @ Edmunds.com:\n",
      "Employees think it’s a pretty great place to work and some pretty impressive publications think it is too: we have been recognized as one of the best places to work by the Fortune Magazine and Great Places to Work, LA Business Journal (for the last 6 years!), Computerworld, Built in LA and Inc. Magazine. We've also been identified as one of the best workplaces specifically in Technology and also for Diversity and Asian Americans. If you’re interested in learning more and joining our mission, we’d love to hear from you!\n",
      "Edmunds will consider for employment qualified candidates with criminal histories in a manner consistent with the requirements of all applicable laws.\n",
      "#LI-POST\n",
      "\n",
      "Flexible work from home options available.\n",
      "----------------------------------------\n",
      "Job Title: Data Engineer\n",
      "Company: Adobe\n",
      "Location: San Jose, CA 95110 \n",
      "(Downtown area)\n",
      "Job Link: https://www.indeed.com/rc/clk?jk=b0d406f04980ecba&bb=csTtYRd9oU4o9SJxQgVHM_eiTaGMZFZyWsZFMS2RTpVz6fKnZLKmUKhojgDVlnd0sG0UiGpUQYw-25ofDwAGQ7-pY12kAhtpAqg9GJVKg-uqhF_ym03VTw%3D%3D&xkcb=SoCp67M3AbjI53ydd50JbzkdCdPP&fccid=f89deb5a97c7738a&vjs=3\n",
      "Details: {}\n",
      "Description: Profile insights\n",
      "Find out how your skills align with the job description\n",
      "Skills\n",
      "Do you have experience in UNIX?\n",
      "Yes\n",
      "No\n",
      "Education\n",
      "Do you have a Bachelor's degree?\n",
      "Yes\n",
      "No\n",
      "Job details\n",
      "Here’s how the job details align with your profile\n",
      ".\n",
      "Pay\n",
      "$108,000 - $198,500 a year\n",
      "Job type\n",
      "Full-time\n",
      "Location\n",
      "San Jose, CA 95110\n",
      "Full job description\n",
      "Our Company\n",
      "\n",
      "Changing the world through digital experiences is what Adobe’s all about. We give everyone—from emerging artists to global brands—everything they need to design and deliver exceptional digital experiences! We’re passionate about empowering people to create beautiful and powerful images, videos, and apps, and transform how companies interact with customers across every screen.\n",
      "\n",
      "We’re on a mission to hire the very best and are committed to creating exceptional employee experiences where everyone is respected and has access to equal opportunity. We realize that new ideas can come from everywhere in the organization, and we know the next big idea could be yours!\n",
      "\n",
      "Adobe is seeking a Data Engineer who consistently demonstrates talent and motivation to join our exceptional Marketing Operations and Engineering department in San Jose. The Marketing Operations and Engineering team is part of Growth Marketing and Insights (GMI) organization and is responsible for the data architecture, transformation, and engineering needs of the entire organization. This includes managing backend data for various marketing measurement tools and processes, including attribution, product and customer segmentation, lifetime value, and more.\n",
      "\n",
      "Responsibilities:\n",
      "Design, develop, and maintain scalable and reliable data pipelines and infrastructure.\n",
      "Collaborate with teams from different departments to gather and analyze data requirements.\n",
      "Optimize data storage, processing, and retrieval for efficient and flawless operation.\n",
      "Implement data models and schemas to support various data-driven applications.\n",
      "Ensure data quality, integrity, and security throughout the data lifecycle.\n",
      "Perform data analysis and provide insights to drive informed decision-making.\n",
      "Keep yourself informed about the latest developments and effective approaches in data engineering.\n",
      "\n",
      "Requirements:\n",
      "Bachelor's degree in Computer Science, Engineering, or a related field, with at least 3 years of data engineering experience.\n",
      "Proven experience in designing and developing data pipelines and ETL processes.\n",
      "Strong programming skills in Python, Java, or Scala.\n",
      "Experience with Spark, Databricks, Airflow and related Big Data technologies\n",
      "Familiarity with cloud-based data platforms (e.g., AWS, Azure, GCP).\n",
      "Knowledge of SQL and database systems.\n",
      "Knowledge of UNIX and shell scripting\n",
      "Strong problem-solving and analytical abilities.\n",
      "Excellent communication and collaboration skills.\n",
      "Experience in marketing domain is a plus\n",
      "\n",
      "Join Adobe, a company that values diversity, inclusion, and equal opportunity. We are an equal opportunity employer and do not discriminate based on gender, race, color, ethnicity, national origin, age, disability, religion, sexual orientation, gender identity or expression, veteran status, or any other applicable characteristics protected by law. At Adobe, we believe that diverse perspectives drive innovation and creativity.\n",
      "\n",
      "Note: If you have a disability or special need that requires accommodation during the application process, please contact us at accommodations@adobe.com or call (408) 536-3015. We aim to ensure our website and application process are accessible to all users.\n",
      "\n",
      "We strictly adhere to fair competition practices and maintain a free and open marketplace for all employees. We have policies in place to ensure that we do not enter into illegal agreements with other companies to not recruit or hire each other's employees.\n",
      "Our compensation reflects the cost of labor across several U.S. geographic markets, and we pay differently based on those defined markets. The U.S. pay range for this position is $108,000 -- $198,500 annually. Pay within this range varies by work location and may also depend on job-related knowledge, skills, and experience. Your recruiter can share more about the specific salary range for the job location during the hiring process.\n",
      "\n",
      "At Adobe, for sales roles starting salaries are expressed as total target compensation (TTC = base + commission), and short-term incentives are in the form of sales commission plans. Non-sales roles starting salaries are expressed as base salary and short-term incentives are in the form of the Annual Incentive Plan (AIP).\n",
      "\n",
      "In addition, certain roles may be eligible for long-term incentives in the form of a new hire equity award.\n",
      "\n",
      "Adobe is proud to be an Equal Employment Opportunity and affirmative action employer. We do not discriminate based on gender, race or color, ethnicity or national origin, age, disability, religion, sexual orientation, gender identity or expression, veteran status, or any other applicable characteristics protected by law. Learn more.\n",
      "\n",
      "Adobe aims to make Adobe.com accessible to any and all users. If you have a disability or special need that requires accommodation to navigate our website or complete the application process, email accommodations@adobe.com or call (408) 536-3015.\n",
      "\n",
      "Adobe values a free and open marketplace for all employees and has policies in place to ensure that we do not enter into illegal agreements with other companies to not recruit or hire each other’s employees.\n",
      "----------------------------------------\n",
      "Job Title: Data Engineer\n",
      "Company: FutureSoft IT\n",
      "Location: Sunnyvale, CA 94043\n",
      "Job Link: https://www.indeed.com/rc/clk?jk=c2d066606e0a58c7&bb=csTtYRd9oU4o9SJxQgVHM85NvxXJmx1t4yOqlpfFp1Vgi_cw71IJqP2R67sIzbQCG8nrzFfZDv8o4HsudY8N7yLQVCeWEcC1PTKBdgmTTEJNLYOWpsnZ4lDF0MA-95nK&xkcb=SoAd67M3AbjI53ydd50IbzkdCdPP&fccid=b6b9755638f54ed0&vjs=3\n",
      "Details: {}\n",
      "Description: Profile insights\n",
      "Find out how your skills align with the job description\n",
      "Skills\n",
      "Do you have experience in Spark?\n",
      "Yes\n",
      "No\n",
      "Location\n",
      "Sunnyvale, CA 94043\n",
      "Full job description\n",
      "**Please Read**\n",
      "\n",
      "\n",
      "Local candidates only. This opportunity does not provide Visa sponsorship. No corp to corp applicants please. Candidate must be available to work on our W2.\n",
      "\n",
      "\n",
      "Data Engineer\n",
      "\n",
      "\n",
      "Data applications are critical to our success, powering many aspects of our marketplace and supporting products. We are looking for data engineers who will build, migrate and maintain data pipelines. In this role, you’ll expand and refactor the data sets that generate and transform data into applications, insights, and experiences for our users.\n",
      "\n",
      "\n",
      "The work includes:\n",
      "? Refactoring existing and build new data pipelines\n",
      "? Migrating existing data sets into next-gen reporting frameworks and tools\n",
      "? Using existing data tools and frameworks to configure reports and metrics\n",
      "? Developing and automating large scale, high-performance data processing systems to drive our business growth and improve the product experience\n",
      "? Building and refactoring scalable data pipelines on top of Hive and Spark leveraging Airflow scheduler/executor framework\n",
      "\n",
      "\n",
      "We are looking for engineers with:\n",
      "? Demonstrated ability to analyze large data sets to identify gaps and inconsistencies, provide data insights, and drive effective product solutions\n",
      "? Experience designing and deploying high performance systems with robust monitoring and logging practices\n",
      "? Experience building high performance data pipelines\n",
      "? Nice to have: proven ability to think critically about team direction and use analysis to inform that\n",
      "? Experience using machine learning is a plus, but not required.\n",
      "? Excellent communication skills, both written and verbal\n",
      "----------------------------------------\n",
      "Job Title: Data Engineer\n",
      "Company: G3 Enterprises\n",
      "Location: Modesto, CA 95358\n",
      "Job Link: https://www.indeed.com/rc/clk?jk=d67f0ff74ef78791&bb=csTtYRd9oU4o9SJxQgVHM-kRti1glr7eunLMhYfpGr7lKieWMKN3mqvqEknGZm2ZjugFZwJvyInQbtnCU01d4T367Fvxxox2JAovUkbSaa7YT7U21ZbyHgv8pbhCOu78&xkcb=SoCT67M3AbjI53ydd50PbzkdCdPP&fccid=ce6464fb3913e7f7&vjs=3\n",
      "Details: {}\n",
      "Description: Profile insights\n",
      "Find out how your skills align with the job description\n",
      "Skills\n",
      "Do you have experience in Spark?\n",
      "Yes\n",
      "No\n",
      "Education\n",
      "Do you have a High school diploma or GED?\n",
      "Yes\n",
      "No\n",
      "Job details\n",
      "Here’s how the job details align with your profile\n",
      ".\n",
      "Pay\n",
      "$42.84 - $53.00 an hour\n",
      "Job type\n",
      "Full-time\n",
      "Location\n",
      "502 E Whitmore Ave, Modesto, CA 95358\n",
      "Full job description\n",
      "G3 Enterprises Job Applicant Privacy Notice\n",
      "\n",
      "Why G3?\n",
      "G3 Enterprises is an industry leader in packaging, logistics, real estate and minerals solutions. Our diverse portfolio of businesses create a variety of opportunities for career growth. We believe in fully leveraging the talent within our organization, presenting employees with challenging work, opportunities for job rotations, special projects, and a changing landscape. Apply Today! We Exist for You to Succeed.\n",
      "\n",
      "Location: Onsite Modesto, CA\n",
      "Full Time W-2 employee\n",
      "\n",
      "Summary\n",
      "The Data Engineer plays a key role in the IT department, supporting the delivery of high-quality data solutions to our business. Responsible for creating and managing the entire sequence of activities in the ETL process (Extract, Transform, and Load) while data moves from source to target location. Supports the development and implementation of data processing and transformation workflows that enable efficient data analysis and reporting. Collaborates with other IT professionals and business stakeholders to integrate data systems and processes into the overall infrastructure. Supports efforts to ensure that data is stored securely and in compliance with data privacy regulations. Monitors and troubleshoots data systems and pipelines to ensure that they are running smoothly and efficiently.\n",
      "\n",
      "Essential Functions\n",
      "This job description reflects management’s assignment of essential functions; it does not prescribe or restrict the tasks that may be assigned.\n",
      "Create and enhance data solutions enabling seamless delivery of data.\n",
      "Develop, implement, and optimize ETL processes and transformation workflows, ensuring processes are scalable, repeatable, and secure for stakeholder needs.\n",
      "Develop data pipelines and data ingestion solutions that process and extract meaning out of data.\n",
      "Collect, parse, manage, and analyze large sets of data across different domains for analysis.\n",
      "Develop processes that make data more easily accessible in real-time, and across user skill levels.\n",
      "Partners with stakeholders to understand the data challenges experienced by the organization, then make recommendations for the development of data solutions, ETL process improvement, and data efficiency enhancement.\n",
      "Troubleshoot and resolve ETL job failures and data discrepancies.\n",
      "Documents data flow diagrams, security access, data quality, and data availability across all business systems.\n",
      "\n",
      "Supervisory Responsibilities\n",
      "N/A\n",
      "\n",
      "Qualifications\n",
      "To perform this job successfully, an individual must be able to perform each essential duty satisfactorily. The requirements listed below are representative of the knowledge, skill and ability required. Reasonable accommodations may be made to enable individuals with disabilities to perform the essential functions.\n",
      "\n",
      "Minimum Qualifications\n",
      "High school diploma or State-issued equivalency certificate.\n",
      "Bachelor's degree plus 2 years of experience as an ETL developer/engineer, Data Engineer, and/or focused on database programming and design.\n",
      "Strong programming skills in languages such as Python, Scala, Java, and SQL.\n",
      "Knowledge of data privacy regulations and best practices for data security.\n",
      "Excellent problem-solving skills and attention to detail.\n",
      "Strong collaboration and communication skills, with the ability to work effectively in a team environment.\n",
      "Strong analytical and critical thinking skills.\n",
      "Skilled in MS Word and PowerPoint at the basic level; skilled in MS Excel at the intermediate level.\n",
      "\n",
      "\n",
      "Preferred Qualifications\n",
      "Bachelor's degree in Computer Science, Information Technology, Math, or Computer Engineering.\n",
      "4 years of experience as an ETL developer/engineer or Data Engineer.\n",
      "Strong understanding of ETL principles and how to apply them within a data lake environment.\n",
      "Experience with big data technologies such as Apache Spark, Hadoop, Docker, Kubernetes, and CI/CD\n",
      "Familiarity with data integration and enterprise integration patterns.\n",
      "Technologically curious with an appetite to learn new systems and processes.\n",
      "\n",
      "Physical Demands\n",
      "The physical demands described here are representative of those that must be met by an employee to successfully perform the essential functions of this job. Reasonable accommodations may be made to enable individuals with disabilities to perform the essential functions.\n",
      "While performing the duties of this job, the employee is frequently required to sit, use hands to finger, handle, feel, talk and hear. The employee is occasionally required to stand and walk.\n",
      "\n",
      "\n",
      "Work Environment\n",
      "The work environment characteristics described here are representative of those an employee encounters while performing the essential functions of this job. Reasonable accommodations may be made to enable individuals with disabilities to perform the essential functions.\n",
      "The noise level in the work environment is usually quiet.\n",
      "\n",
      "Compensation\n",
      "Hiring Hourly Rate: $42.84 - $53,56\n",
      "Actual compensation amount paid may be lower or higher to be determined by factors other than race and gender such as the education, experience, knowledge, skills and abilities of the applicant, internal equity, and alignment with market data.\n",
      "Perks & Benefits\n",
      "This position includes a competitive benefits package.\n",
      "\n",
      "EEO/AA M/F/Vet/Disability\n",
      "----------------------------------------\n",
      "Job Title: Data Warehousing Engineer\n",
      "Company: BayOne\n",
      "Location: Mountain View, CA\n",
      "Job Link: https://www.indeed.com/rc/clk?jk=ae92ad67e534a1fe&bb=csTtYRd9oU4o9SJxQgVHM8FksbNImoG8h7EVxGch47GF398sWhOyu6N4jFBSwUlSPteNBCq52afB16iOZnPU-6Yo8ulG9kebkkbnu0xorb8akMXlnBzltWcveyjKj5UA&xkcb=SoAn67M3AbjI53ydd50ObzkdCdPP&fccid=f7b2d477df13840f&vjs=3\n",
      "Details: {}\n",
      "Description: Job details\n",
      "Here’s how the job details align with your profile\n",
      ".\n",
      "Job type\n",
      "Contract\n",
      "Location\n",
      "Mountain View, CA\n",
      "Full job description\n",
      "Data Warehousing Engineers\n",
      "\n",
      "Engage with business leaders to gather requirements and translate to scalable data warehousing solutions:\n",
      "\n",
      "Design storage, schema & data models using Cloud data warehouse technologies (F1, Plx. BigQuery)\n",
      "\n",
      "Build security models, data infrastructure, system integration in compliance with Cloud access policies\n",
      "\n",
      "Create data sets and business metrics\n",
      "\n",
      "Build data pipelines & automated workflows\n",
      "\n",
      "Design Looker LookML models and complex dashboards\n",
      "\n",
      "Deliver data solutions using Agile methodology\n",
      "\n",
      "Design storage, schema & data models using Cloud data warehouse technologies (F1, Plx. Big Query)\n",
      "----------------------------------------\n",
      "Job Title: Cloud Data Platform Engineer\n",
      "Company: Apple\n",
      "Location: Cupertino, CA\n",
      "Job Link: https://www.indeed.com/rc/clk?jk=f5009c786a033083&bb=csTtYRd9oU4o9SJxQgVHMwVRHYKRHCUIDExDRBAotKgXSKOO3kVIFEyDRKj8gy6DvDPiovoDggFHuJ3kFqvxAQ1swebRdfVttcCmsvKPewZKiCL3tPGG9g%3D%3D&xkcb=SoC667M3AbjI53ydd50NbzkdCdPP&fccid=c1099851e9794854&vjs=3\n",
      "Details: {}\n",
      "Description: Profile insights\n",
      "Find out how your skills align with the job description\n",
      "Skills\n",
      "Do you have experience in Snowflake?\n",
      "Yes\n",
      "No\n",
      "Education\n",
      "Do you have a Bachelor's degree?\n",
      "Yes\n",
      "No\n",
      "Job details\n",
      "Here’s how the job details align with your profile\n",
      ".\n",
      "Pay\n",
      "$138,900 - $256,500 a year\n",
      "Job type\n",
      "Full-time\n",
      "Location\n",
      "Cupertino, CA\n",
      "Benefits\n",
      "Pulled from the full job description\n",
      "Dental insurance\n",
      "Employee stock purchase plan\n",
      "Health insurance\n",
      "RSU\n",
      "Retirement plan\n",
      "Full job description\n",
      "Summary\n",
      "\n",
      "Posted: May 20, 2024\n",
      "\n",
      "Weekly Hours: 40\n",
      "\n",
      "Role Number:200508564\n",
      "\n",
      "The people here at Apple don’t just build products - we craft the kind of wonder that’s revolutionized entire industries. It’s the diversity of those people and their ideas that supports the innovation that runs through everything we do, from amazing technology to industry-leading environmental efforts. Join Apple, and help us leave the world better than we found it. Would you like to be a part of one of the world’s fastest growing data warehouses for the world’s largest company? Does the prospect of being challenged with sophisticated database design and optimization problems excite you? The Global Business Intelligence (GBI) team within Apple's IS&T organization has a very large Enterprise Data Warehouse (EDW) to support analytical and reporting needs of hundreds of global Apple users. This is an extraordinary opportunity for an inquisitive, experienced and results-oriented database expert to work on our EDW platform to provide a scalable, high-performance and active data-warehousing platform. You are self-driven, highly motivated, innovative, have a good work ethic and have a consistent track record of designing and leading large, complex data warehouses. The position will involve close interaction with development, product, database administrators and support teams. This person will review database designs to ensure that they are optimized for performance, scalability and be a thought leader and champion in automation. This role will provide second-level support to address any platform wide performance and stability issues.\n",
      "\n",
      "Description\n",
      "\n",
      "Participation in Physical Model reviews with the development teams to provide inputs on improving performance Continuous analysis of the database environment to capitalize on system-wide tuning opportunities Act as an inspiring leader and conduct brown bag lunch sessions for development teams to create awareness regarding the value proposition of new and upcoming features Performance optimization of database resources intensive ETL and individual user queries Drive new features testing, rollout and `adoption Workload Management analysis and improvement to meet response-time SLA's as well as optimize resource consumption Influence, establish best engineering practices through solid design decisions, processes and automation\n",
      "\n",
      "Minimum Qualifications\n",
      "\n",
      "Minimum Qualifications\n",
      "\n",
      "Key Qualifications\n",
      "3+ years experience in relational cloud based database technologies like Snowflake\n",
      "Ability to identify system level bottlenecks in the database and recommend solutions\n",
      "Object oriented programming experience in Python/Java or Scala\n",
      "Analyze production workloads and develop strategies to run Snowflake database with scale and efficiency\n",
      "Experience in Snowflake performance tuning, capacity planning, managing cloud spend and utilization is a plus\n",
      "Manage all aspects of database security both at infrastructure and application level\n",
      "Ability to articulate and enforce Snowflake best practices and find ingenious way of enforcing them\n",
      "Manage Cross regional data replication and be an expert at database recovery features\n",
      "Ability to define and automate DBA functions\n",
      "Experience with AWS or GCP or Azure cloud specifically in compute , storage and security services\n",
      "\n",
      "Preferred Qualifications\n",
      "\n",
      "Preferred Qualifications\n",
      "\n",
      "Education & Experience\n",
      "\n",
      "BS in Computer Science or equivalent\n",
      "\n",
      "Additional Requirements\n",
      "\n",
      "Pay & Benefits\n",
      "At Apple, base pay is one part of our total compensation package and is determined within a range. This provides the opportunity to progress as you grow and develop within a role. The base pay range for this role is between $138,900 and $256,500, and your base pay will depend on your skills, qualifications, experience, and location. Apple employees also have the opportunity to become an Apple shareholder through participation in Apple’s discretionary employee stock programs. Apple employees are eligible for discretionary restricted stock unit awards, and can purchase Apple stock at a discount if voluntarily participating in Apple’s Employee Stock Purchase Plan. You’ll also receive benefits including: Comprehensive medical and dental coverage, retirement benefits, a range of discounted products and free services, and for formal education related to advancing your career at Apple, reimbursement for certain educational expenses - including tuition. Additionally, this role might be eligible for discretionary bonuses or commission payments as well as relocation. Learn more about Apple Benefits. Note: Apple benefit, compensation and employee stock programs are subject to eligibility requirements and other terms of the applicable plan or program.\n",
      "\n",
      "More\n",
      "Apple is an equal opportunity employer that is committed to inclusion and diversity. We take affirmative action to ensure equal opportunity for all applicants without regard to race, color, religion, sex, sexual orientation, gender identity, national origin, disability, Veteran status, or other legally protected characteristics. Learn more about your EEO rights as an applicant.\n",
      "----------------------------------------\n",
      "Job Title: Data Engineer\n",
      "Company: Punch Agency\n",
      "Location: San Francisco, CA\n",
      "Job Link: https://www.indeed.com/rc/clk?jk=beff3db2294873de&bb=csTtYRd9oU4o9SJxQgVHM_PmSxZ7v4SzDI_3cgCRWNnpvxE4nsHNRCmCSTG_vs1_mvEY_V_YmaW3Pa3YAQFn7oGunvK0jM0aALaCaWPjs4u44-3NgMRsDdQX0RRX2tSu&xkcb=SoAO67M3AbjI53ydd50MbzkdCdPP&fccid=b6ebf4abe55e972f&vjs=3\n",
      "Details: {}\n",
      "Description: Profile insights\n",
      "Find out how your skills align with the job description\n",
      "Skills\n",
      "Do you have experience in Spark?\n",
      "Yes\n",
      "No\n",
      "Education\n",
      "Do you have a Bachelor's degree?\n",
      "Yes\n",
      "No\n",
      "Job details\n",
      "Here’s how the job details align with your profile\n",
      ".\n",
      "Job type\n",
      "Full-time\n",
      "Location\n",
      "San Francisco, CA\n",
      "Full job description\n",
      "Company Description\n",
      "\n",
      "Punch is a full service digital agency based out of New York, Silicon Valley, LA, Boston, Stockholm and Phuket, Thailand. Here at Punch, we take pride in tackling our clients’ most challenging tasks. Founded by a group of former Google Engineers and now with over 75 staff, Punch is now leading the way in creating new cutting-edge technology.\n",
      "We're looking for innovative thinkers with a hunger to expand their knowledge!\n",
      "We are building a more technical world.Go ahead and check out some of our work here: www.punch-agency.com\n",
      "\n",
      "Job Description\n",
      "\n",
      "We are seeking an extremely talented senior/lead data engineer to design & implement highly reliable & scalable Audience Management Big Data Platform (500+ Terabytes).\n",
      "Responsibilities:\n",
      "Build, enhance and scale our existing Big Data Platform to process 500+ Terabytes of data from various 3rd party providers.\n",
      "Technically own the complete data pipeline module including data ingestion, validation, transformation & storage in the data platform.\n",
      "Design and write clean, robust, server-side code using Spark & Scala.\n",
      "Perform R & D to evaluate new technologies, generate different ideas & patentable IP to communicate value for company\n",
      "Work closely with team members & mentor them technically\n",
      "Partner with product managers to support product roadmap & business needs\n",
      "Required Skills:\n",
      "- 10 - 15 years experience in software development primarily in Big Data domain using Spark & Hadoop eco system.\n",
      "5+ Years of experience in Spark & Scala (Skill Level: 8 or more out of 10)\n",
      "Bachelors degree in Computer Science or equivalent.\n",
      "5+ years experience designing, building and launching extremely efficient & reliable data pipelines to move data (both large and small amounts) using modern data architecture/tools.\n",
      "Strong background (5+ years) in ETL processing for large amount of data (250+ Terabytes)\n",
      "Multiple years of experience working with 250+ Terabytes of data for ETL using Spark & Scala\n",
      "3+ years experience working in Agile environment (SCRUM)\n",
      "Nice To Have Skills:\n",
      "Experience in digital marketing (Good Domain knowledge on digital marketing Industry)\n",
      "Experience working with AWS (S3, EMR, EC2, RDS)\n",
      "Job Type: Full-time\n",
      "Required education:\n",
      "Bachelor's\n",
      "Required experience:\n",
      "designing, building: 5 years\n",
      "Job Type: Full-time\n",
      "Required education:\n",
      "Bachelor's\n",
      "Required experience:\n",
      "designing, building: 5 years\n",
      "\n",
      "Qualifications\n",
      "\n",
      "Required Skills:\n",
      "- 10 - 15 years experience in software development primarily in Big Data domain using Spark & Hadoop eco system.\n",
      "5+ Years of experience in Spark & Scala (Skill Level: 8 or more out of 10)\n",
      "Bachelors degree in Computer Science or equivalent.\n",
      "5+ years experience designing, building and launching extremely efficient & reliable data pipelines to move data (both large and small amounts) using modern data architecture/tools.\n",
      "Strong background (5+ years) in ETL processing for large amount of data (250+ Terabytes)\n",
      "Multiple years of experience working with 250+ Terabytes of data for ETL using Spark & Scala\n",
      "3+ years experience working in Agile environment (SCRUM)\n",
      "Nice To Have Skills:\n",
      "Experience in digital marketing (Good Domain knowledge on digital marketing Industry)\n",
      "Experience working with AWS (S3, EMR, EC2, RDS)\n",
      "Job Type: Full-time\n",
      "Required education:\n",
      "Bachelor's\n",
      "Required experience:\n",
      "designing, building: 5 years\n",
      "Job Type: Full-time\n",
      "Required education:\n",
      "Bachelor's\n",
      "Required experience:\n",
      "designing, building: 5 years\n",
      "\n",
      "Additional Information\n",
      "\n",
      "We are a team of former Google Engineers and Designers based in Silicon Valley, New York and Phuket, Thailand.\n",
      "All of our core engineers are paid in the $150,000 range or greater and we also provide our engineers the opportunity to work, all expenses paid, from our Phuket, Thailand office one month a year as a creative outlet to work and enjoy life to the fullest.\n",
      "In this role you will work with some of the most exciting projects in the industry today with a team of fast moving engineers and designers who have a passion for building disruptive products.\n",
      "----------------------------------------\n",
      "Job Title: Data Engineer\n",
      "Company: ABOTTS\n",
      "Location: Oakley, CA\n",
      "Job Link: https://www.indeed.com/rc/clk?jk=c0d48594262052f6&bb=csTtYRd9oU4o9SJxQgVHMyEjB-fKq0gOe-W_owy6NPK5qBI5CNb_-41aMAyZhIExWnbIpE_rA4_zJ-moQnYWkUX_xgr1nEUIMy2mfj_RPhp2_y8_V480PYBr7NHC3uzm&xkcb=SoDn67M3AbjI53ydd50DbzkdCdPP&fccid=eeffccef0a07d204&vjs=3\n",
      "Details: {}\n",
      "Description: Profile insights\n",
      "Find out how your skills align with the job description\n",
      "Skills\n",
      "Do you have experience in Snowflake?\n",
      "Yes\n",
      "No\n",
      "Job details\n",
      "Here’s how the job details align with your profile\n",
      ".\n",
      "Pay\n",
      "$105,000 a year\n",
      "Job type\n",
      "Full-time\n",
      "Location\n",
      "Oakley, CA\n",
      "Full job description\n",
      "Job duties:\n",
      "Conduct thorough and consistent evaluations of data pipeline performance, employing a meticulous approach to ensure efficiency, reliability, and optimal throughput at all stages of data integration and processing Harness best-in-class data quality frameworks for accurate diagnostics, troubleshooting, and to fortify data integrity, leveraging tools and services provided by Snowflake and AWS Display agility in adapting to diverse SQL environments, tapping into cloud-native capabilities, especially within platforms like Amazon Web Services. Demonstrate expertise in utilizing tools like DBeaver and ODBC for efficient data management and processing, including seamless integration with Snowflake Take charge of all performance metrics, setting, and maintaining standards for reliability, accuracy, and alignment with key performance indicators. Uphold the highest data governance protocols, emphasizing secure accessibility, pinpoint accuracy, and adherence to industry standards. Engage in active collaborations with various stakeholders, leveraging tools like JIRA to manage tasks and ensure seamless workflows aligned with Agile practices. Reference industry benchmarks, using tools like Excel for data visualization, to drive insights and identify avenues for improvement. Document, interpret, and utilize Python for advanced data analysis, data pipelines, bridging complex datasets with meaningful business decisions. Elevate data manipulation and retrieval processes by integrating advanced SQL operations and leveraging modern architectures. Champion end-to-end data management, analysis, and visualization, utilizing the best tools in the market for comprehensive data solutions. Prioritize robust data security, using advanced encryption, and data compression techniques, to safeguard data in multi-cloud setups, ensuring seamless integrations and transfers. Continually refine operational strategies, staying in tune with emerging industry trends, technological innovations, and evolving business needs. Commit to ongoing learning and tool mastery, ensuring the team is always equipped with the latest knowledge and resources to tackle contemporary challenges.\n",
      "Required qualifications:\n",
      "Engineering\n",
      "Position Title: Data Engineer\n",
      "Developer Salary: 105000\n",
      "Job Type: FULL TIME\n",
      "Location: Oakley, CA\n",
      "TX Job ID: JD1037\n",
      "----------------------------------------\n",
      "Job Title: Data Engineer\n",
      "Company: HEALTHCARE FINANCE DIRECT LLC\n",
      "Location: Remote in California\n",
      "Job Link: https://www.indeed.com/rc/clk?jk=64a044173a2e1376&bb=csTtYRd9oU4o9SJxQgVHM8CA9QsT6t0Iw2JxnoHI1bAUs9JrkYAyT03Y8HBqyl6CkzxqxMp1WEXbO-VNdxT_-EsKhQuNNFKzQCOebB3Y04eDuyjandLY4uTEYa9d1ive&xkcb=SoBT67M3AbjI53ydd50CbzkdCdPP&fccid=dd616958bd9ddc12&vjs=3\n",
      "Details: {}\n",
      "Description: Profile insights\n",
      "Find out how your skills align with the job description\n",
      "Skills\n",
      "Do you have experience in Tableau?\n",
      "Yes\n",
      "No\n",
      "Education\n",
      "Do you have a Bachelor's degree?\n",
      "Yes\n",
      "No\n",
      "Job details\n",
      "Here’s how the job details align with your profile\n",
      ".\n",
      "Pay\n",
      "$95,000 - $135,000 a year\n",
      "Job type\n",
      "Full-time\n",
      "Full job description\n",
      "Description:\n",
      "\n",
      "Are you EPIC?\n",
      "Do you have the ability to demonstrate, understand and apply HFD’s core purpose and\n",
      "values in all that you do? At HFD our core purpose is to help the underserved live healthier. In order to accomplish this mission, we must ensure that our team is aligned with our E.P.I.C. values:\n",
      "? Excellence: Always exceeding expectations!\n",
      "? Passionate: Executing with boldness!\n",
      "? Innovative: Pioneering a better way!\n",
      "? Collaborative: Together we win!\n",
      "The EPIC Data Engineer we are looking for:\n",
      "As a Data Engineer, you will be responsible for building and maintaining our data pipeline infrastructure, optimizing data workflows and implementing efficient data storage solutions. You will be responsible for optimizing reports and delivering data up stream and work closely with the IT team to enforce data quality standards over vast amounts of data. You will report to the CTO but will work closely with stakeholders to improve data streams across the organization for analytics and data science purposes.\n",
      "Conducts complex, important work under minimal supervision and with wide latitude for independent judgment. Typically requires a bachelor's degree (or international equivalent) and 6+ years of relevant experience.\n",
      "Requirements:\n",
      "\n",
      "As a Data Engineer, you will:\n",
      "Build and maintain scalable and efficient data processing pipelines, using various technologies such SSIS.\n",
      "Develop and maintain monitoring tools to ensure the health and reliability of our data infrastructure.\n",
      "Optimize poor performing queries.\n",
      "Determine, enforce and document database policies, procedures and standards.\n",
      "Perform tests and evaluations regularly to ensure data security, privacy and integrity.\n",
      "Monitor database performance, implement changes and apply new patches and versions when required.\n",
      "Ensure that all data-related processes comply with relevant data privacy and security regulations.\n",
      "Collaborate with stakeholders to understand reporting requirements and performance expectations.\n",
      "Define and implement data structures to support reporting needs.\n",
      "Requirements:\n",
      "Bachelor’s Degree in Computer Science or related category\n",
      "6+ years of industry specific experience\n",
      "Ability to read, write, and speak the English language proficiently.\n",
      "Proven working experience with Microsoft SQL Server, MongoDB, and Postgres SQL\n",
      "Proven experience with SSIS and SSRS\n",
      "Proven Experience with Tableau reporting/ building\n",
      "Hands-on experience with database standards and end user applications\n",
      "Familiarity with database design, documentation and SQL coding\n",
      "Experience with cloud-based data storage solutions such as AWS S3, and Azure Blob Storage.\n",
      "Strong understanding of data modeling, data warehousing, and ETL processes.\n",
      "Problem solving skills and ability to think algorithmically.\n",
      "----------------------------------------\n",
      "Job Title: Data Engineer\n",
      "Company: BXGI\n",
      "Location: Remote in Palo Alto, CA\n",
      "Job Link: https://www.indeed.com/rc/clk?jk=2af08b861cf28136&bb=csTtYRd9oU4o9SJxQgVHM7YQfdn2ay1DZxkuAN6CgVpegIayjrb-Bq80IZQg6SRsaPUyef56nKldtlkRcZ4sFBqe9wGLuLARWiyPFKK-_lUCVBElOkZRawt8lb8weyZB&xkcb=SoDO67M3AbjI53ydd50BbzkdCdPP&fccid=359847eb6abf10b7&vjs=3\n",
      "Details: {}\n",
      "Description: Profile insights\n",
      "Find out how your skills align with the job description\n",
      "Skills\n",
      "Do you have experience in Tableau?\n",
      "Yes\n",
      "No\n",
      "Education\n",
      "Do you have a Bachelor's degree?\n",
      "Yes\n",
      "No\n",
      "Full job description\n",
      "Overview\n",
      "We are conducting a search for a Data Engineer to join our client’s engineering team in Palo Alto, CA or remotely anywhere in the United States.\n",
      "Interested in shaping the future of pediatrics behavioral health? Our client is looking for a skilled Data Engineer to join their Engineering team who has in-depth experience with a variety of databases and maintaining data repositories from multiple data sources using different schemas.\n",
      "Come join a world-class company delivering AI-powered diagnostics and therapeutics to dramatically improve the outlook of young children with cognitive and behavioral conditions.\n",
      "Ability to work remotely is possible, with quarterly visits to Palo Alto.\n",
      "\n",
      "Responsibilities\n",
      "Create and maintain a data repository for analytics by combining multiple data sources using different schemas.\n",
      "Work with various groups in Engineering to source the necessary data.\n",
      "Implement industry best practices including quality assurance and compliance standards related to Protected Health Information (HIPAA).\n",
      "Develop and maintain ETL processes to populate the data warehouse.\n",
      "Deploy utility applications to Amazon AWS EC2, Fargate or Kubernetes.\n",
      "Qualifications\n",
      "BS in Computer Science, Engineering or related discipline.\n",
      "4+ years experience with two of MySQL, Oracle SQL, Postgres.\n",
      "Deep understanding of relational database structure and performance.\n",
      "Past experience maintaining data repositories with disparate data sources.\n",
      "Past experience building visualizations of underlying data.\n",
      "Experience with Linux, Python, REST API.\n",
      "Excellent verbal and written communication skills.\n",
      "Excellent organizational skills and attention to detail.\n",
      "Nice to Have\n",
      "Experience with Amazon Web Services (RDS)\n",
      "Experience building/supporting HIPAA-compliant software.\n",
      "Understanding of underlying technologies of Tableau (Hyper databases, Hyper API, server extracts, prep flows, Python libraries.\n",
      "Deep understanding of dimensional data warehouses\n",
      "Experience with design star and snowflake schemas\n",
      "Ability to come into the office every so often (once we actually can).\n",
      "\n",
      "If interested, please send your resume to lana@bxgi.com.\n",
      "----------------------------------------\n",
      "Job Title: Data Engineer II- Remote, USA\n",
      "Company: Ambry Genetics Corporation\n",
      "Location: Remote in Aliso Viejo, CA 92656\n",
      "Job Link: https://www.indeed.com/rc/clk?jk=abd1b8acadd8601e&bb=csTtYRd9oU4o9SJxQgVHM1yU3LT0S5kOHgZNpbNO76aVyBZ8Qb-Gas2958g6wlBZ5NK7uzeLYxb6WVQDUZHmJA8xISOAEbblt1nT1gRvXgdUL3Tl6b_QHWFWv58w8JUX&xkcb=SoB667M3AbjI53ydd50AbzkdCdPP&fccid=7e68666c11b85ab8&vjs=3\n",
      "Details: {}\n",
      "Description: Profile insights\n",
      "Find out how your skills align with the job description\n",
      "Skills\n",
      "Do you have experience in Visio?\n",
      "Yes\n",
      "No\n",
      "Education\n",
      "Do you have a Master's degree?\n",
      "Yes\n",
      "No\n",
      "Job details\n",
      "Here’s how the job details align with your profile\n",
      ".\n",
      "Pay\n",
      "$100,000 - $145,000 a year\n",
      "Job type\n",
      "Full-time\n",
      "Benefits\n",
      "Pulled from the full job description\n",
      "401(k) 4% Match\n",
      "Dental insurance\n",
      "Flexible spending account\n",
      "Health insurance\n",
      "Paid sick time\n",
      "Paid time off\n",
      "Vision insurance\n",
      "Full job description\n",
      "Compensation: $100,000-145,000 per year. You are eligible to a Short-Term Incentive Plan with the target at 7.5% of your annual earnings, terms and conditions apply\n",
      "Data Engineer II – Remote, USA\n",
      "As a Data Engineer at Ambry you’ll approach tasks with a customer-based, cloud-first mindset to support and enhance various data platform products, including Ambry’s data lakes, streams, and warehouses. This role will be primarily responsible for building, monitoring, and operationalizing our data streams which are hydrated via CDC (change data capture) from a suite of ~20 on-prem and cloud databases.\n",
      "Essential Functions\n",
      "Build Kafka connectors to sync updates from source data stores\n",
      "Build partitioned Kafka topics to sync updates to destination data marts\n",
      "Build multiplexed data analytics workloads using Apache Flink to monitor streaming metrics and perform real-time data transformations\n",
      "Build dashboards using Datadog and Cloudwatch to ensure system health and user support\n",
      "Build opinionated but accommodating schema registries that ensure data governance\n",
      "Work closely with your West Coast based scrum team to submit and review PRs daily, maintain documentation and backlogs, validate builds across multiple environments, and deploy at a 2-4 week sprint cadence\n",
      "Consult with other scrum teams to align on agreed upon data contracts, API specifications, and coordinate deployment schedules\n",
      "Design reasonable database schemas with query access patterns as the forethought\n",
      "Build and maintain CI/CD pipelines using infrastructure-as-code\n",
      "Iteratively migrate on-prem ETL jobs written in PHP into AWS Flink and Glue processes\n",
      "Partner with QA Engineers in building automated test suites\n",
      "Partner with end-users to resolve service disruptions and evangelize our data product offerings\n",
      "Vigilantly oversee data quality and alert upstream data producers of all disparities, latency, and defects\n",
      "Other duties as assigned\n",
      "Qualifications\n",
      "Strong familiarity with any combination of our tech stacks in order of importance: Apache Kafka (MSK flavor preferred), Debezium, Python, Apache Flink or PySpark Streaming, MySQL (RDS flavors preferred), Python, CDK or Terraform, Athena, Glue, Lambda, Appflow, HANA/4, PHP, Redis, Docker, Javascript\n",
      "At least six years’ experience working with professional scrum teams and/or equivalent schooling\n",
      "At least four years’ experience using Git versioning control\n",
      "At least three years’ experience designing and indexing relational databases\n",
      "At least two years’ experience building and operationalizing real-time data streams\n",
      "At least one year experience in building dashboards for system monitoring and error logging\n",
      "Preferred\n",
      "Master’s degree in computer, data, math, or life sciences\n",
      "Basic understanding of genomic concepts and terminology\n",
      "Willing to work PST hours between 8:00 AM - 5:00 PM or 9:00 AM – 6:00 PM\n",
      "Experience building data APIs and offering Data as a Service\n",
      "Experience integrating with SaaS platforms such as SAP and Salesforce\n",
      "Experience or willingness to learn working with PHP MVC frameworks such as Symfony\n",
      "Experience with Atlassian products, i.e. Jira, Confluence, Bamboo\n",
      "Experience with system diagramming tools such as Miro, LucidCharts, or Visio\n",
      "About Us:\n",
      "Ambry Genetics Corporation is a CAP-accredited and CLIA-licensed molecular genetics laboratory based in Aliso Viejo, California. We are a genetics-based healthcare company that is dedicated to open scientific exchange so we can work together to understand and treat all human disease faster.\n",
      "At Ambry, everyone is welcome. A career at Ambry Genetics is a chance to be part of a dynamic company that aims to improve health by understanding the relationships between genetics and human disease. We earned our reputation as industry leaders by responsibly introducing cutting-edge genetic testing solutions and continually sharing what we learn with the global scientific community.\n",
      "At Ambry you will be learning, challenging yourself, and having fun while collaborating with teammates through the open exchange of ideas. Our outstanding benefits program includes medical, dental, vision, 401k with a 4% employer match, FSA, paid sick leave and generous paid time off (PTO) program. You can learn more about the benefits here. Ambry Genetics is an Equal Opportunity Employer (EOE) and we maintain a drug-free work environment.\n",
      "Our salary ranges are determined by role, level, and location. The range displayed on each job posting reflects the minimum and maximum target for new hire salaries for the position across all US locations. Within the range, individual pay is determined by work location and additional factors, including job-related skills, experience, and relevant education or training.\n",
      "All qualified applicants will receive consideration for employment without regard to race (and traits historically associated with race, including, but not limited to hair texture and protective hairstyles such as braids, locks, and twists), color, creed, religion, sex, sexual orientation, gender identity, gender expression (including transgender status), national origin, ancestry, age, marital status or protected veteran status and will not be discriminated against on the basis of disability, protected medical condition as defined by applicable state or local law, genetic information, or any other characteristic protected by applicable federal, state, or local laws and ordinances. If you have a disability or special need that requires accommodation, please contact us at careers@ambrygen.com\n",
      "Ambry does not accept unsolicited resumes from individual recruiters, third party recruiting agencies, outside recruiters or firms without an executed contract in place. We are not responsible for any fees related to resumes that are unsolicited or are received by Ambry. Such resumes will be deemed the sole property of Ambry and will be processed accordingly.\n",
      "PRIVACY NOTICES\n",
      "To review Ambry’s Privacy Notice, Click here: https://www.ambrygen.com/legal/privacy-policy\n",
      "To review the California privacy notice, click here: California Privacy Notice | Ambry Genetics\n",
      "To review the UKG privacy notice, click here: California Privacy Notice | UKG\n",
      "\n",
      "#LI-REMOTE #LI-NK1\n",
      "----------------------------------------\n",
      "Job Title: DATA ENGINEER\n",
      "Company: Big Bright International\n",
      "Location: Irvine, CA\n",
      "Job Link: https://www.indeed.com/rc/clk?jk=3ff49794581b1faa&bb=csTtYRd9oU4o9SJxQgVHM5m_22M3386oje1m5fn-dH5ulbCQwZ3oe-XB1MI2u6mncpJaq_LefmQzJMBZYCEUNZov7U_B4yiCzu7mI0LcCRs91m8GxizlmnaF_ykUJ1i9&xkcb=SoD067M3AbjI53ydd50HbzkdCdPP&fccid=bd17e04d65337121&vjs=3\n",
      "Details: {}\n",
      "Description: Profile insights\n",
      "Find out how your skills align with the job description\n",
      "Skills\n",
      "Do you have experience in Apache Hive?\n",
      "Yes\n",
      "No\n",
      "Location\n",
      "Irvine, CA\n",
      "Full job description\n",
      "Data Engineer at various unanticipated client locations throughout the US to provide design, development, testing and implementation for business computer systems; provide ETL big data integration; utilize Data Stage to populate tables in data warehouse and data marts; design hive schema, create hive tables, and load and analyze data using hive queries.\n",
      "----------------------------------------\n",
      "Job Title: Data Engineer (Databricks)\n",
      "Company: EHUB GLOBAL\n",
      "Location: Santa Clara, CA 95050\n",
      "Job Link: https://www.indeed.com/rc/clk?jk=d7b732f905fdfd79&bb=csTtYRd9oU4o9SJxQgVHM0xHuDgnxdHvDt_eNqA5C8pV3PrAJJVgN85vjnlwksBP5dn0TxnJpcusNpDfs03rcIPgI-8cet9GC4kvKHjjlAMgNZ0Nlqn4hrtrxypNWGvI&xkcb=SoBA67M3AbjI53ydd50GbzkdCdPP&fccid=5be7603de6743699&cmp=ehub-global&ti=Data+Engineer&vjs=3\n",
      "Details: {}\n",
      "Description: Profile insights\n",
      "Find out how your skills align with the job description\n",
      "Certifications\n",
      "Do you have a valid Databricks Certified Data Engineer Professional certification?\n",
      "Yes\n",
      "No\n",
      "Skills\n",
      "Do you have experience in Spark?\n",
      "Yes\n",
      "No\n",
      "Education\n",
      "Do you have a Bachelor's degree?\n",
      "Yes\n",
      "No\n",
      "Job details\n",
      "Here’s how the job details align with your profile\n",
      ".\n",
      "Pay\n",
      "From $62 an hour\n",
      "Job type\n",
      "Contract\n",
      "Shift and schedule\n",
      "8 hour shift\n",
      "Monday to Friday\n",
      "Location\n",
      "2003 Acacia Court, Santa Clara, CA 95050\n",
      "Benefits\n",
      "Pulled from the full job description\n",
      "401(k)\n",
      "Dental insurance\n",
      "Health insurance\n",
      "Full job description\n",
      "Job Title: Data Engineer (Data Bricks & Backend)\n",
      "Location: Santa Clara, CA (onsite)\n",
      "Duration:12+months\n",
      "11 YEARS\n",
      "Overview:\n",
      "Seeking a Data Engineer with expertise in DataBricks, strong SQL skills, and exposure to project management principles. The role involves designing, building, and optimizing data pipelines while also supporting project coordination and planning activities. Ideal for candidates with technical prowess and a collaborative mindset.\n",
      "Key Responsibilities:\n",
      "Develop and maintain data pipelines using DataBricks for efficient data processing.\n",
      "Utilize SQL to query, transform, and analyze data from various sources.\n",
      "Assist in project planning, scheduling, and resource allocation.\n",
      "Collaborate with cross-functional teams to ensure data engineering aligns with project objectives.\n",
      "Troubleshoot data pipeline issues and implement solutions for improved performance.\n",
      "Support data analysis initiatives by providing clean, curated datasets to analysts and data scientists.\n",
      "Qualifications:\n",
      "Proficiency in DataBricks for building and optimizing data pipelines.\n",
      "Strong SQL skills for data querying, transformation, and analysis.\n",
      "Exposure to project management methodologies and coordination.\n",
      "Bachelor's degree in Computer Science, Engineering, or related field (preferred).\n",
      "Excellent communication and teamwork skills.\n",
      "Preferred Skills:\n",
      "Experience with cloud platforms (e.g., AWS, Azure, GCP).\n",
      "Familiarity with big data frameworks (e.g., Apache Spark).\n",
      "Knowledge of data warehousing concepts and tools.\n",
      "Ability to adapt to evolving technology trends and project requirements.\n",
      "Job Type: Contract\n",
      "Pay: From $62.00 per hour\n",
      "Expected hours: 40 per week\n",
      "Benefits:\n",
      "401(k)\n",
      "Dental insurance\n",
      "Health insurance\n",
      "Compensation package:\n",
      "Hourly pay\n",
      "Experience level:\n",
      "11+ years\n",
      "Schedule:\n",
      "8 hour shift\n",
      "Monday to Friday\n",
      "Experience:\n",
      "DataBricks: 10 years (Required)\n",
      "SQL: 10 years (Required)\n",
      "Project management: 2 years (Required)\n",
      "Ability to Commute:\n",
      "Santa Clara, CA 95050 (Required)\n",
      "Ability to Relocate:\n",
      "Santa Clara, CA 95050: Relocate before starting work (Required)\n",
      "Work Location: In person\n",
      "----------------------------------------\n",
      "Error processing job: Message: stale element reference: stale element not found\n",
      "  (Session info: chrome=125.0.6422.77); For documentation on this error, please visit: https://www.selenium.dev/documentation/webdriver/troubleshooting/errors#stale-element-reference-exception\n",
      "Stacktrace:\n",
      "0   chromedriver                        0x000000010061a510 chromedriver + 4302096\n",
      "1   chromedriver                        0x0000000100612e58 chromedriver + 4271704\n",
      "2   chromedriver                        0x000000010024419c chromedriver + 278940\n",
      "3   chromedriver                        0x0000000100251d50 chromedriver + 335184\n",
      "4   chromedriver                        0x0000000100249414 chromedriver + 300052\n",
      "5   chromedriver                        0x0000000100247cf8 chromedriver + 294136\n",
      "6   chromedriver                        0x000000010024a4e8 chromedriver + 304360\n",
      "7   chromedriver                        0x000000010024a560 chromedriver + 304480\n",
      "8   chromedriver                        0x00000001002863c0 chromedriver + 549824\n",
      "9   chromedriver                        0x000000010027c85c chromedriver + 510044\n",
      "10  chromedriver                        0x00000001002bec5c chromedriver + 781404\n",
      "11  chromedriver                        0x000000010027b004 chromedriver + 503812\n",
      "12  chromedriver                        0x000000010027b9ec chromedriver + 506348\n",
      "13  chromedriver                        0x00000001005e2558 chromedriver + 4072792\n",
      "14  chromedriver                        0x00000001005e7004 chromedriver + 4091908\n",
      "15  chromedriver                        0x00000001005c979c chromedriver + 3970972\n",
      "16  chromedriver                        0x00000001005e78ec chromedriver + 4094188\n",
      "17  chromedriver                        0x00000001005bc71c chromedriver + 3917596\n",
      "18  chromedriver                        0x0000000100604b50 chromedriver + 4213584\n",
      "19  chromedriver                        0x0000000100604ccc chromedriver + 4213964\n",
      "20  chromedriver                        0x0000000100612a50 chromedriver + 4270672\n",
      "21  libsystem_pthread.dylib             0x00000001956daf94 _pthread_start + 136\n",
      "22  libsystem_pthread.dylib             0x00000001956d5d34 thread_start + 8\n",
      "\n"
     ]
    }
   ],
   "source": [
    "base_url = 'https://www.indeed.com/jobs?q=data+engineer&l=California'\n",
    "num_pages = 3\n",
    "\n",
    "for page in range(num_pages):\n",
    "    url = f'{base_url}&start={page * 10}'\n",
    "    driver.get(url)\n",
    "    time.sleep(3)  # wait for the page to load\n",
    "\n",
    "    # Find all job postings\n",
    "    job_postings = driver.find_elements(By.CLASS_NAME, 'job_seen_beacon')\n",
    "\n",
    "    # Extract information from each job posting\n",
    "    for job in job_postings:\n",
    "        try:\n",
    "            # Click the job to view details\n",
    "            job.click()\n",
    "            time.sleep(2)  # wait for the details panel to load\n",
    "\n",
    "            # Extract job title\n",
    "            title_elem = job.find_element(By.CLASS_NAME, 'jobTitle')\n",
    "            title = title_elem.text.strip() if title_elem else 'N/A'\n",
    "            \n",
    "            # Extract company name    \n",
    "            company_elem = job.find_element(By.CSS_SELECTOR, \"span[data-testid='company-name']\")\n",
    "            company = company_elem.text.strip() if company_elem else 'N/A'\n",
    "            \n",
    "            # Extract job location\n",
    "            location_elem = job.find_element(By.CSS_SELECTOR, \"div[data-testid='text-location']\")\n",
    "            location = location_elem.text.strip() if location_elem else 'N/A'\n",
    "            \n",
    "            # Extract job link\n",
    "            joblink_elem = job.find_element(By.CLASS_NAME, 'jobTitle').find_element(By.CLASS_NAME, 'jcs-JobTitle').get_attribute('href')\n",
    "            joblink = joblink_elem.strip() if joblink_elem else 'N/A'\n",
    "\n",
    "            # Extract job details (e.g., salary, employment type, etc.)\n",
    "            detail_sections = driver.find_elements(By.CLASS_NAME, 'jobsearch-JobDescriptionSection-sectionItem')\n",
    "            details = {}\n",
    "            for section in detail_sections:\n",
    "                key_elem = section.find_element(By.CLASS_NAME, 'jobsearch-JobDescriptionSection-sectionItemKey')\n",
    "                value_elem = section.find_element(By.CLASS_NAME, 'jobsearch-JobDescriptionSection-sectionItemValue')\n",
    "                key = key_elem.text.strip() if key_elem else 'N/A'\n",
    "                value = value_elem.text.strip() if value_elem else 'N/A'\n",
    "                details[key] = value\n",
    "\n",
    "            # Extract job description\n",
    "            description_elem = driver.find_element(By.CLASS_NAME, 'jobsearch-JobComponent-description')\n",
    "            description = description_elem.text.strip() if description_elem else 'N/A'\n",
    "            \n",
    "            # Print job information\n",
    "            print(f'Job Title: {title}')\n",
    "            print(f'Company: {company}')\n",
    "            print(f'Location: {location}')\n",
    "            print(f'Job Link: {joblink}')\n",
    "            print(f'Details: {details}')\n",
    "            print(f'Description: {description}')\n",
    "            print('-' * 40)\n",
    "\n",
    "            # Create a dictionary to store the job data\n",
    "            job_data = {\n",
    "                'title': title,\n",
    "                'company': company,\n",
    "                'location': location,\n",
    "                'job_link': joblink,\n",
    "                'details': details,\n",
    "                'description': description\n",
    "            }\n",
    "            \n",
    "            # Insert the job data into MongoDB\n",
    "            collection.insert_one(job_data)\n",
    "        except Exception as e:\n",
    "            print(f\"Error processing job: {e}\")\n",
    "            continue"
   ]
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-08T20:44:04.124507Z",
     "start_time": "2024-06-08T20:44:04.099241Z"
    }
   },
   "cell_type": "code",
   "source": "driver.find_element(By.CSS_SELECTOR, 'div[data-testid=\"jobsearch-OtherJobDetailsContainer\"]').find_element(By.CSS_SELECTOR, 'div[id=\"salaryInfoAndJobType\"]').text.strip()",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'$78,700 - $145,100 a year - Full-time'"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 20
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
